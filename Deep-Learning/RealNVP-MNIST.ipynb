{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e4f01-e885-46f8-b046-27d67f94303a",
   "metadata": {},
   "source": [
    "# RealNVP\n",
    "\n",
    "Real-Valued Non-Volume Preserving Transformation- is the flow-based deep neural network, which is a tytpe of a probabilistic generative models that uses a sequence of invertible transformations to model complex data distributions. \n",
    "\n",
    "Some key characteristics are:\n",
    "- Use bijective(invertible) functions, allowing efficien computation of forward of data to latent space and reverse transformation to data space. This means no information will be lost during the recovering from the latent space\n",
    "- Use of Jacobian Determinant which is essential for handle dimensionality changes without losing data.\n",
    "\n",
    "Applied areas:\n",
    "- Density Estimation and Anomaly Detection. The exact likelihood computation makes it easier to identify anomalies\n",
    "- Image and Vide generation\n",
    "- Reversible data compression\n",
    "- Data augmentation for cases when there are lack of original data\n",
    "- Bayesian Inference and Probabillists modeling when uncertanty needs to be incorporated  \n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366c782-fed2-4676-be96-3c7bfdf0b13f",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5ff92-f8fc-4e6b-9ae6-86910c406a2d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63fb9b70-2de1-47b8-b302-6297e79ca0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    regularizers,\n",
    "    models,\n",
    "    metrics,\n",
    "    optimizers,\n",
    "    callbacks,\n",
    "    datasets\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bc77b-6188-44e8-bbf1-94ecbd7efac2",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de443289-6f27-4064-8c38-2520670f7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(images, n=10, fig_size=(20, 3), c_map='gray_r', as_type='float32', save_to=None):\n",
    "    if save_to is not None:\n",
    "        oFull_path = Path(save_to)\n",
    "        \n",
    "        oFull_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    if images.max() > 1.0:\n",
    "        images = images / 255.0\n",
    "    elif images.min() < 0.0:\n",
    "        images = (images + 1.0) / 2.0\n",
    "\n",
    "    plt.figure(figsize=fig_size)\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.imshow(images[i].astype(as_type), cmap=c_map)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Сохранение изображения\n",
    "    if save_to:\n",
    "        plt.savefig(save_to)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_mnist_imgs(images, labels, n=5, show_random=False):\n",
    "    '''\n",
    "    Display images from MNIST dataset\n",
    "    Args:\n",
    "        images (ndarray): MNIST images\n",
    "        labesls (ndarray): MNIST labels\n",
    "        n (int): Number of images to display\n",
    "        show_random (bool): Select images randonly or not\n",
    "\n",
    "    Returns:\n",
    "        Generates images from MNIST dataset in accrordance with given parameters\n",
    "    '''\n",
    "    \n",
    "    if show_random:\n",
    "        img_idx= np.random.choice(images.shape[0], n, replace=False)\n",
    "    else:\n",
    "        img_idx= range(0, n)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for i, idx in enumerate(img_idx):\n",
    "        plt.subplot(2,n,i+1)\n",
    "        plt.imshow(images[idx], cmap='gray')\n",
    "        plt.title(f'Label: {labels[idx]}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Discretize function to convert [0, 1] range to [0, 255] (integers)\n",
    "def discretize(sample):\n",
    "    return tf.cast(sample * 255, tf.int32)\n",
    "\n",
    "def show_imgs(imgs, title=None, row_size=4):\n",
    "    # Convert images to NumPy array if they are in TensorFlow tensor format\n",
    "    if isinstance(imgs, tf.Tensor):\n",
    "        imgs = imgs.numpy()\n",
    "    \n",
    "    num_imgs = imgs.shape[0]\n",
    "    is_int = imgs.dtype == np.int32\n",
    "    nrow = min(num_imgs, row_size)\n",
    "    ncol = int(math.ceil(num_imgs / nrow))\n",
    "    \n",
    "    # Normalize pixel values if they are integers\n",
    "    if is_int:\n",
    "        imgs = np.clip(imgs, 0, 255)\n",
    "        imgs = imgs.astype(np.uint8)\n",
    "    else:\n",
    "        imgs = np.clip(imgs, 0, 1)\n",
    "\n",
    "    # Arrange images in a grid\n",
    "    grid_img = []\n",
    "    for i in range(ncol):\n",
    "        row_imgs = imgs[i * nrow:(i + 1) * nrow]\n",
    "        if len(row_imgs) < nrow:\n",
    "            # Pad row with blank images if necessary\n",
    "            padding = nrow - len(row_imgs)\n",
    "            pad_img = np.zeros_like(row_imgs[0])\n",
    "            row_imgs = np.concatenate([row_imgs, np.array([pad_img] * padding)])\n",
    "        grid_img.append(np.concatenate(row_imgs, axis=1))\n",
    "    \n",
    "    grid_img = np.concatenate(grid_img, axis=0)\n",
    "\n",
    "    # Plot the grid\n",
    "    plt.figure(figsize=(1.5 * nrow, 1.5 * ncol))\n",
    "    plt.imshow(grid_img, interpolation='nearest', cmap='gray' if is_int else None)\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea4800-be10-4d92-a257-b9652aab248e",
   "metadata": {},
   "source": [
    "## KEARS MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516b672-311b-4320-a277-dbad42dc144a",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62da9d00-b9c9-485c-9762-826e5825e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info= tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a8be5b1-8eaf-4576-833a-6a05fbb28e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33123726-bc6f-4217-a7a2-a5862df372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744613d-4ac3-450d-b9e6-47e8f9d182c4",
   "metadata": {},
   "source": [
    "### BUILD RealNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c546d9e-12e8-4f8f-bf34-f9b16294662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coupling_MNIST(input_shape, coupling_dim, reg):\n",
    "   \n",
    "    input_layer = layers.Input(shape=(input_shape))\n",
    "\n",
    "    # Sсale component architecture\n",
    "    # firtst 4 layers have relu activation which helps to learn non-linearity, as well as squize and scale data flow\n",
    "    # tanh activation on last layer for stability purposes\n",
    "    s_layer_1= layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(input_layer)\n",
    "\n",
    "    s_layer_2=layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(s_layer_1)\n",
    "\n",
    "    s_layer_3= layers.Flatten()(s_layer_2)\n",
    "    s_layer_4= layers.Dense(\n",
    "        coupling_dim,\n",
    "        activation='relu',\n",
    "        kernel_regularizer= regularizers.l2(reg)\n",
    "    )(s_layer_3)\n",
    "\n",
    "    # We use a tanh activation multiplied by a learned scale parameter. This is presumably to mitigate \n",
    "    # the effect of using exp(s) to scale the variables. Directly using the outputs of a neural network could cause \n",
    "    # big swings in s leading to blowing up exp(s). \n",
    "    # To same point as above we also add a small L2 regularization on s\n",
    "    s_layer_output= layers.Dense(\n",
    "        input_shape[0] * input_shape[1], \n",
    "        activation='tanh',\n",
    "        kernel_regularizer= regularizers.l2(reg)\n",
    "    )(s_layer_4)\n",
    "\n",
    "    # Note the number of output vars are equal to input\n",
    "    s_layer_output= layers.Reshape(\n",
    "        (\n",
    "            input_shape[0],\n",
    "            input_shape[1]\n",
    "        )\n",
    "    )(s_layer_output)\n",
    "\n",
    "    # Shift component architecture\n",
    "    # relu activation uses for to learn non-linearity purposes\n",
    "    # last layer uses linear activation as we need to shift our data flow with no restrictions\n",
    "    t_layer_1= layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(reg)\n",
    "    )(input_layer)\n",
    "\n",
    "    t_layer_2= layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layer_1)\n",
    "\n",
    "    t_layer_3= layers.Flatten()(t_layer_2)\n",
    "\n",
    "    t_layer_4= layers.Dense(\n",
    "        coupling_dim,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layer_3)\n",
    "\n",
    "    t_layer_output= layers.Dense(\n",
    "        input_shape[0] * input_shape[1],\n",
    "        activation='linear',\n",
    "        kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layers_4)\n",
    "\n",
    "    t_layer_output= layers.Reshape(\n",
    "        (\n",
    "            input_shape[0],\n",
    "            input_shape[1]\n",
    "        )\n",
    "    )(t_layer_output)\n",
    "\n",
    "    return models.Model(inputs=input_layer, outputs=[s_layer_5, t_layer_5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e76d9-2b6e-4868-8db7-a27fa6dacf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(models.Model):\n",
    "    def __init__(self, input_dim, coupling_layers, coupling_dim, regularization):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.coupling_layers = coupling_layers\n",
    "        # Configure a multivariate normal distribution with zero mean and unit variance.\n",
    "        # This is used as the target distribution in the latent space, to which the model will map input data.\n",
    "        self.distribution = tfp.distributions.MultivariateNormalDiag(loc=[0.0, 0.0], scale_diag=[1.0, 1.0])\n",
    "\n",
    "        # Configure masks used to construct the coupling layers. Masks alternate between [0, 1] and [1, 0],\n",
    "        # allowing some components to be transformed while others remain unchanged at each layer.\n",
    "        self.masks = np.array([[0, 1], [1, 0]] * (coupling_layers // 2), dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Set up a metric to track the average loss during training.\n",
    "        self.loss_tracker = metrics.Mean(name='loss')\n",
    "\n",
    "        # Initialize the list of coupling layers with the given input dimension, layer width, and regularization.\n",
    "        self.flows = [Coupling_MNIST(input_dim, coupling_dim, regularization) for i in range(coupling_layers)]\n",
    "\n",
    "        self.input_dim=input_dim\n",
    "       \n",
    "       \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "   # This is X data which will be passed to the flow f(x)(i.e. Coupling) and z returned in the end\n",
    "    def call(self, x, training=True):\n",
    "        '''\n",
    "        Forward pass function that computes the model's output and the log-determinant of the Jacobian.\n",
    "        '''\n",
    "      \n",
    "        # Variable to accumulate the sum of log-determinants of the Jacobian, necessary for density estimation.\n",
    "        log_det_inv = 0\n",
    "\n",
    "        # Set the direction of flow: reverse during training (-1) to map data to the latent distribution,\n",
    "        # and forward (1) during inference to sample from the latent space.\n",
    "        direction = -1 if training else 1\n",
    "\n",
    "        for i in range(self.flows)[::direction]:\n",
    "            # Mask the data using the i-th mask to select components for transformation.\n",
    "            x_masked = x * self.masks[i]\n",
    "            \n",
    "            # Reverse the mask to select the components that will remain unchanged.\n",
    "            # The reversed_mask itself is a tensor that alternates between values of 0 and 1, \n",
    "            # meaning some parts of s and t will effectively be multiplied by 0, setting them to zero for specific components,\n",
    "            # it used to control which parts of s and t are applied to x in each step.\n",
    "            reversed_mask = 1 - self.masks[i]\n",
    "\n",
    "            # Obtain the scale (s) and shift (t) factors from the i-th coupling layer.\n",
    "            # These parameters apply a non-linear transformation to masked components.\n",
    "            s, t = self.flows[i](x_masked)\n",
    "           \n",
    "            # Ensure only masked components are transformed by setting other components of s and t to zero.\n",
    "            # Here, s *= reversed_mask and t *= reversed_mask serve to \"zero out\" certain elements in s and t \n",
    "            # for specific parts of the input, based on which components are masked at each layer.\n",
    "            # This ensures that when we later apply the affine transformation x * exp(s) + t, \n",
    "            # it only affects the selected components and leaves the others unchanged.\n",
    "            s *= reversed_mask\n",
    "            t *= reversed_mask\n",
    "           \n",
    "            \n",
    "            # Calculate the gate coefficient for adjusting transformation scaling direction.\n",
    "            gate = (direction - 1) / 2\n",
    "            \n",
    "            # Update the data by applying an affine transformation, keeping part of x unchanged and transforming the rest.\n",
    "            z = (reversed_mask * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s)) + x_masked)\n",
    "            \n",
    "            # Accumulate the log-determinant Jacobian, critical for computing probability densities.\n",
    "            ldj += gate * tf.reduce_sum(s, axis=1)\n",
    "\n",
    "        # return latent space z if trainig True, and recovered data if False\n",
    "        return z, ldj\n",
    "\n",
    "    def log_loss(self, x):\n",
    "        '''\n",
    "        Calculates the log-likelihood loss function.\n",
    "        '''\n",
    "\n",
    "        # Call method call via self. Transform input x to output y and compute the log-determinant of the Jacobian (logdet).\n",
    "        y, ldj = self(x)\n",
    "        # Compute log-likelihood in latent space by combining the log probability and log determinant.\n",
    "        log_likelihood = self.distribution.log_prob(y) + ldj\n",
    "\n",
    "        # Return the negative mean log-likelihood (for loss minimization).\n",
    "        return -tf.reduce_mean(log_likelihood)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Record operations for automatic differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.log_loss(data)\n",
    "\n",
    "        # Compute gradients and update the model's trainable variables.\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Update the tracked loss metric with the current loss value.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "      \n",
    "       \n",
    "        return {'loss': self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Calculate and track the loss in evaluation mode.\n",
    "        loss = self.log_loss(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {'loss': self.loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f15f6a-5534-4d13-ae1a-df52819dbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dequantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733edac-43b1-4d5a-947b-280a846e03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RealNVP(\n",
    "    input_dim=(28, 28, 1),\n",
    "    coupling_layers=8,  # Можно начать с 8 и потом увеличить\n",
    "    coupling_dim=256,\n",
    "    regularization=0.01\n",
    ")\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49493e99-d7db-4d6d-a876-a12924576611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "class ImageFlow(tf.keras.Model):\n",
    "    def __init__(self, flows, import_samples=8):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            flows - A list of flows (each a tf.keras.Layer) to be applied on images.\n",
    "            import_samples - Number of importance samples for testing.\n",
    "        \"\"\"\n",
    "        super(ImageFlow, self).__init__()\n",
    "        self.flows = flows\n",
    "        self.import_samples = import_samples\n",
    "        self.prior = tfd.Normal(loc=0.0, scale=1.0)  # Prior distribution\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        \"\"\"\n",
    "        Given a batch of images, return the latent representation z and ldj of transformations.\n",
    "        \"\"\"\n",
    "        z = imgs\n",
    "        ldj = tf.zeros([tf.shape(imgs)[0]])  # Initialize log-det-jacobian\n",
    "        for flow in self.flows:\n",
    "            z, ldj = flow(z, ldj, reverse=False)\n",
    "        return z, ldj\n",
    "\n",
    "    def _get_likelihood(self, imgs, return_ll=False):\n",
    "        \"\"\"\n",
    "        Given a batch of images, return the likelihood.\n",
    "        If return_ll is True, returns log-likelihood.\n",
    "        Otherwise, returns bits per dimension (bpd).\n",
    "        \"\"\"\n",
    "        z, ldj = self.encode(imgs)\n",
    "        log_pz = tf.reduce_sum(self.prior.log_prob(z), axis=[1, 2, 3])  # Sum over all dimensions\n",
    "        log_px = ldj + log_pz\n",
    "        nll = -log_px\n",
    "        # Bits per dimension calculation\n",
    "        bpd = nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "        return tf.reduce_mean(bpd) if not return_ll else log_px\n",
    "\n",
    "    def sample(self, img_shape, z_init=None):\n",
    "        \"\"\"\n",
    "        Generate a batch of images by inverting the flows.\n",
    "        \"\"\"\n",
    "        # Sample latent representation from prior\n",
    "        if z_init is None:\n",
    "            z = self.prior.sample(sample_shape=img_shape)\n",
    "        else:\n",
    "            z = z_init\n",
    "        ldj = tf.zeros([img_shape[0]])\n",
    "        # Reverse the flows to transform z to x\n",
    "        for flow in reversed(self.flows):\n",
    "            z, ldj = flow(z, ldj, reverse=True)\n",
    "        return z\n",
    "\n",
    "    def call(self, imgs):\n",
    "        # Forward function used to calculate likelihood during training\n",
    "        return self._get_likelihood(imgs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        imgs, _ = data  # Unpack data (images, labels)\n",
    "        with tf.GradientTape() as tape:\n",
    "            bpd = self._get_likelihood(imgs)\n",
    "        grads = tape.gradient(bpd, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(bpd)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        imgs, _ = data\n",
    "        # Importance sampling\n",
    "        samples = []\n",
    "        for _ in range(self.import_samples):\n",
    "            img_ll = self._get_likelihood(imgs, return_ll=True)\n",
    "            samples.append(img_ll)\n",
    "        img_ll = tf.stack(samples, axis=-1)\n",
    "        img_ll = tf.reduce_logsumexp(img_ll, axis=-1) - np.log(self.import_samples)\n",
    "        # Calculate bpd\n",
    "        bpd = -img_ll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "        self.compiled_metrics.update_state(bpd)\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a7a8f29-1379-4661-b7da-b4aa46ac0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем слой Dequantization (предполагается, что он уже определен)\n",
    "class Dequantization(tf.keras.layers.Layer):\n",
    "    def __init__(self, alpha=1e-5, quants=256, **kwargs):\n",
    "        super(Dequantization, self).__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.quants = quants\n",
    "\n",
    "    def call(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, ldj = self.dequant(z, ldj)\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=True)\n",
    "        else:\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=False)\n",
    "            z = z * self.quants\n",
    "            ldj += np.log(self.quants) * tf.reduce_prod(tf.shape(z)[1:])\n",
    "            z = tf.clip_by_value(tf.floor(z), 0, self.quants - 1)\n",
    "            z = tf.cast(z, tf.int32)\n",
    "        return z, ldj\n",
    "\n",
    "    def sigmoid(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            ldj += tf.reduce_sum(-z - 2 * tf.nn.softplus(-z), axis=[1, 2, 3])\n",
    "            z = tf.sigmoid(z)\n",
    "            ldj -= np.log(1 - self.alpha) * tf.reduce_prod(tf.shape(z)[1:])\n",
    "            z = (z - 0.5 * self.alpha) / (1 - self.alpha)\n",
    "        else:\n",
    "            z = z * (1 - self.alpha) + 0.5 * self.alpha\n",
    "            ldj += np.log(1 - self.alpha) * tf.reduce_prod(tf.shape(z)[1:])\n",
    "            ldj += tf.reduce_sum(-tf.math.log(z) - tf.math.log(1 - z), axis=[1, 2, 3])\n",
    "            z = tf.math.log(z) - tf.math.log(1 - z)\n",
    "        return z, ldj\n",
    "\n",
    "    def dequant(self, z, ldj):\n",
    "        # Преобразуем z в float32 и добавляем шум\n",
    "        z = tf.cast(z, tf.float32)\n",
    "        z = z + tf.random.uniform(tf.shape(z), minval=0, maxval=1, dtype=tf.float32)\n",
    "        z = z / self.quants\n",
    "        \n",
    "        # Преобразуем результат tf.reduce_prod к float32 для согласованности типов\n",
    "        ldj -= tf.cast(np.log(self.quants), tf.float32) * tf.cast(tf.reduce_prod(tf.shape(z)[1:]), tf.float32)\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8400ae0d-c787-4263-87da-92dd108b0ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Dequantization.call().\n\n\u001b[1mcannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: \u001b[0m\n\nArguments received by Dequantization.call():\n  • z=tf.Tensor(shape=(1, 28, 28), dtype=float32)\n  • ldj=tf.Tensor(shape=(1,), dtype=float32)\n  • reverse=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 33\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully inverted dequantization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Поскольку точная инверсия не гарантируется из-за ограничений точности float\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# tf.debugging.assert_equal(orig_img, reconst_img)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Запуск теста\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtest_dequantization_invertibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 15\u001b[0m, in \u001b[0;36mtest_dequantization_invertibility\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m dequant_module \u001b[38;5;241m=\u001b[39m Dequantization()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Прямое и обратное деквантование\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m deq_img, ldj \u001b[38;5;241m=\u001b[39m \u001b[43mdequant_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m reconst_img, ldj \u001b[38;5;241m=\u001b[39m dequant_module(deq_img, ldj, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Проверка на несовпадающие значения\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensrnv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[73], line 11\u001b[0m, in \u001b[0;36mDequantization.call\u001b[0;34m(self, z, ldj, reverse)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reverse:\n\u001b[1;32m     10\u001b[0m     z, ldj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(z, ldj)\n\u001b[0;32m---> 11\u001b[0m     z, ldj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     z, ldj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(z, ldj, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[73], line 28\u001b[0m, in \u001b[0;36mDequantization.sigmoid\u001b[0;34m(self, z, ldj, reverse)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mldj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     ldj \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;241m-\u001b[39mtf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(z) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m z), axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     30\u001b[0m     z \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(z) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m z)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Dequantization.call().\n\n\u001b[1mcannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: \u001b[0m\n\nArguments received by Dequantization.call():\n  • z=tf.Tensor(shape=(1, 28, 28), dtype=float32)\n  • ldj=tf.Tensor(shape=(1,), dtype=float32)\n  • reverse=False"
     ]
    }
   ],
   "source": [
    "# Тестируем инвертируемость\n",
    "def test_dequantization_invertibility():\n",
    "    # Инициализируем случайное значение для согласованности результатов\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Загрузка примера изображения (для теста можно использовать произвольное изображение из MNIST)\n",
    "    (x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "    orig_img = tf.expand_dims(tf.cast(x_train[0], tf.float32), axis=0)  # (1, 28, 28)\n",
    "    ldj = tf.zeros((1,), dtype=tf.float32)\n",
    "    \n",
    "    # Инициализация слоя деквантования\n",
    "    dequant_module = Dequantization()\n",
    "    \n",
    "    # Прямое и обратное деквантование\n",
    "    deq_img, ldj = dequant_module(orig_img, ldj, reverse=False)\n",
    "    reconst_img, ldj = dequant_module(deq_img, ldj, reverse=True)\n",
    "    \n",
    "    # Проверка на несовпадающие значения\n",
    "    diff_indices = tf.where(tf.not_equal(orig_img, reconst_img))\n",
    "    if tf.size(diff_indices) != 0:\n",
    "        print(\"Dequantization was not invertible.\")\n",
    "        for i in range(tf.shape(diff_indices)[0]):\n",
    "            original_value = orig_img[0, diff_indices[i][1], diff_indices[i][2]].numpy()\n",
    "            reconstructed_value = reconst_img[0, diff_indices[i][1], diff_indices[i][2]].numpy()\n",
    "            print(f\"Original value: {original_value}, Reconstructed value: {reconstructed_value}\")\n",
    "    else:\n",
    "        print(\"Successfully inverted dequantization\")\n",
    "\n",
    "    # Поскольку точная инверсия не гарантируется из-за ограничений точности float\n",
    "    # tf.debugging.assert_equal(orig_img, reconst_img)\n",
    "\n",
    "# Запуск теста\n",
    "test_dequantization_invertibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d146f1d-7571-4b36-89df-0070f96f1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CouplingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, network, mask, c_in):\n",
    "        \"\"\"\n",
    "        Coupling layer inside a normalizing flow.\n",
    "        Args:\n",
    "            network: A Keras model constituting the deep neural network for `s` and `t`.\n",
    "                     Output shape should be twice the channel size as the input.\n",
    "            mask: Binary mask (0 or 1) where 0 denotes that the element should be transformed,\n",
    "                  while 1 means the latent will be used as input to the network.\n",
    "            c_in: Number of input channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.scaling_factor = tf.Variable(tf.zeros(c_in), trainable=True, dtype=tf.float32)\n",
    "        self.mask = tf.constant(mask, dtype=tf.float32)  # Save mask as a constant tensor\n",
    "\n",
    "    def call(self, z, ldj, reverse=False, orig_img=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the coupling layer.\n",
    "        Args:\n",
    "            z: Latent input to the flow.\n",
    "            ldj: The current ldj of the previous flows. The ldj of this layer will be added to this tensor.\n",
    "            reverse: If True, apply the inverse of the layer.\n",
    "            orig_img (optional): Optional conditioning input, e.g., original image.\n",
    "        \"\"\"\n",
    "        # Apply network to masked input\n",
    "        z_in = z * self.mask\n",
    "        if orig_img is None:\n",
    "            nn_out = self.network(z_in)\n",
    "        else:\n",
    "            nn_out = self.network(tf.concat([z_in, orig_img], axis=-1))\n",
    "\n",
    "        # Split network output into scale (s) and shift (t) parts\n",
    "        s, t = tf.split(nn_out, num_or_size_splits=2, axis=-1)\n",
    "\n",
    "        # Stabilize scaling output\n",
    "        s_fac = tf.reshape(tf.exp(self.scaling_factor), (1, -1, 1, 1))\n",
    "        s = tf.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        # Mask outputs (only transform the second part)\n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "\n",
    "        # Affine transformation\n",
    "        if not reverse:\n",
    "            z = (z + t) * tf.exp(s)\n",
    "            ldj += tf.reduce_sum(s, axis=[1, 2, 3])\n",
    "        else:\n",
    "            z = (z * tf.exp(-s)) - t\n",
    "            ldj -= tf.reduce_sum(s, axis=[1, 2, 3])\n",
    "\n",
    "        return z, ldj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecc5cc5d-d4b6-4769-aafe-6325b71a98f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Подготовим диапазон значений для x и список коэффициентов масштабирования\n",
    "x = tf.range(-5.0, 5.0, 0.01)\n",
    "scaling_factors = [0.5, 1.0, 2.0]\n",
    "\n",
    "# Настроим визуализацию\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "# Построение графиков для каждого коэффициента масштабирования\n",
    "for i, scale in enumerate(scaling_factors):\n",
    "    y = tf.tanh(x / scale) * scale\n",
    "    ax[i].plot(x.numpy(), y.numpy())  # Преобразуем тензоры в numpy для отображения\n",
    "    ax[i].set_title(\"Scaling factor: \" + str(scale))\n",
    "    ax[i].set_ylim(-3, 3)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "sns.reset_orig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acbeb70e-4ceb-432a-be9d-a3fccef3b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, invert=False):\n",
    "    # Создаем сетки индексов\n",
    "    x = tf.range(h, dtype=tf.int32)\n",
    "    y = tf.range(w, dtype=tf.int32)\n",
    "    xx, yy = tf.meshgrid(x, y, indexing='ij')\n",
    "    \n",
    "    # Создаем маску с чередующимися значениями\n",
    "    mask = tf.math.mod(xx + yy, 2)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.reshape(mask, (1, 1, h, w))  # Подгоняем под размер 4D тензора\n",
    "    \n",
    "    # Инвертируем маску при необходимости\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "def create_channel_mask(c_in, invert=False):\n",
    "    # Создаем маску, где половина каналов заполнена единицами, а остальная часть нулями\n",
    "    ones = tf.ones(c_in // 2, dtype=tf.float32)\n",
    "    zeros = tf.zeros(c_in - c_in // 2, dtype=tf.float32)\n",
    "    mask = tf.concat([ones, zeros], axis=0)\n",
    "    mask = tf.reshape(mask, (1, c_in, 1, 1))  # Подгоняем под размер 4D тензора\n",
    "    \n",
    "    # Инвертируем маску при необходимости\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0fd0552-12c6-4741-8676-1f51232eb7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (2, 8, 8) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Визуализация масок\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mshow_imgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckerboard_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCheckerboard mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m show_imgs(tf\u001b[38;5;241m.\u001b[39mtranspose(channel_mask, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChannel mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[82], line 30\u001b[0m, in \u001b[0;36mshow_imgs\u001b[0;34m(imgs, title, row_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_int:\n\u001b[1;32m     29\u001b[0m             img \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m-\u001b[39m img\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (img\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m img\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)  \u001b[38;5;66;03m# нормализуем в диапазоне [0, 1]\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m         \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensrnv/lib/python3.11/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda3/envs/tensrnv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5666\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensrnv/lib/python3.11/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (2, 8, 8) for image data"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Создаем маски с использованием уже адаптированных функций\n",
    "checkerboard_mask = tf.tile(create_checkerboard_mask(h=8, w=8), [1, 2, 1, 1])\n",
    "channel_mask = tf.tile(create_channel_mask(c_in=2), [1, 1, 8, 8])\n",
    "\n",
    "\n",
    "\n",
    "# Визуализация масок\n",
    "show_imgs(tf.transpose(checkerboard_mask, perm=[0, 1, 2, 3]), \"Checkerboard mask\")\n",
    "show_imgs(tf.transpose(channel_mask, perm=[0, 1, 2, 3]), \"Channel mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a176ad-0cea-4729-abe0-de723284beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatELU(layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.concat([tf.nn.elu(x), tf.nn.elu(-x)], axis=-1)\n",
    "\n",
    "class LayerNormChannels(layers.Layer):\n",
    "    def __init__(self, c_in, eps=1e-5):\n",
    "        super(LayerNormChannels, self).__init__()\n",
    "        self.gamma = self.add_weight(\"gamma\", shape=(1, c_in, 1, 1), initializer=\"ones\")\n",
    "        self.beta = self.add_weight(\"beta\", shape=(1, c_in, 1, 1), initializer=\"zeros\")\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "        var = tf.reduce_variance(x, axis=1, keepdims=True)\n",
    "        y = (x - mean) / tf.sqrt(var + self.eps)\n",
    "        return y * self.gamma + self.beta\n",
    "\n",
    "class GatedConv(layers.Layer):\n",
    "    def __init__(self, c_in, c_hidden):\n",
    "        super(GatedConv, self).__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            ConcatELU(),\n",
    "            layers.Conv2D(c_hidden, kernel_size=3, padding='same'),\n",
    "            ConcatELU(),\n",
    "            layers.Conv2D(2 * c_in, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.net(x)\n",
    "        val, gate = tf.split(out, num_or_size_splits=2, axis=-1)\n",
    "        return x + val * tf.sigmoid(gate)\n",
    "\n",
    "class GatedConvNet(layers.Layer):\n",
    "    def __init__(self, c_in, c_hidden=32, c_out=-1, num_layers=3):\n",
    "        super(GatedConvNet, self).__init__()\n",
    "        c_out = c_out if c_out > 0 else 2 * c_in\n",
    "        layers_list = [\n",
    "            layers.Conv2D(c_hidden, kernel_size=3, padding='same')\n",
    "        ]\n",
    "        for _ in range(num_layers):\n",
    "            layers_list += [\n",
    "                GatedConv(c_hidden, c_hidden),\n",
    "                LayerNormChannels(c_hidden)\n",
    "            ]\n",
    "        layers_list += [\n",
    "            ConcatELU(),\n",
    "            layers.Conv2D(c_out, kernel_size=3, padding='same')\n",
    "        ]\n",
    "        \n",
    "        self.nn = tf.keras.Sequential(layers_list)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "def create_simple_flow(use_vardeq=True):\n",
    "    flow_layers = []\n",
    "\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [\n",
    "            CouplingLayer(\n",
    "                network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n",
    "                mask=create_checkerboard_mask(h=28, w=28, invert=(i % 2 == 1)),\n",
    "                c_in=1\n",
    "            ) for i in range(4)\n",
    "        ]\n",
    "        flow_layers.append(VariationalDequantization(var_flows=vardeq_layers))\n",
    "    else:\n",
    "        flow_layers.append(Dequantization())\n",
    "\n",
    "    for i in range(8):\n",
    "        flow_layers.append(\n",
    "            CouplingLayer(\n",
    "                network=GatedConvNet(c_in=1, c_hidden=32),\n",
    "                mask=create_checkerboard_mask(h=28, w=28, invert=(i % 2 == 1)),\n",
    "                c_in=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers)\n",
    "    return flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ede2e-3919-4416-86a9-77a353d86256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "268bfeb8-35e4-4961-84a4-d8306cf702bb",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.make_moons.html\n",
    "https://bjlkeng.io/posts/normalizing-flows-with-real-nvp/#implementation\n",
    "https://ar5iv.labs.arxiv.org/html/1605.08803\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial11/NF_image_modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887d2c1-909f-4e4f-b60f-a76812ae0188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
