{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f97487-f0e7-4464-a62b-94c3cde04af5",
   "metadata": {},
   "source": [
    "# MUSIC GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60398cb3-80e5-4340-b46d-d006180e9111",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f1266-e5c2-4e9c-a99f-f470808b3d30",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c7ebd49-8bcc-417e-9414-9c20269c0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "import os\n",
    "import pickle as pkl\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from typing import List, Tuple, Optional\n",
    "import requests\n",
    "import glob\n",
    "from tensorflow.keras import(\n",
    "    layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede82bc-b197-4995-8c8b-da678f97a3b3",
   "metadata": {},
   "source": [
    "### Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85791840-71c4-412e-9fba-c926b744c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_MIDI_FILES= True\n",
    "PARSED_DATA_PATH='./datasets/bach-cello-parseed/'\n",
    "DATA_PATH='./datasets/bach-cello/'\n",
    "DATASET_REPETTIOTIONS= 1\n",
    "SEQ_LEN= 50\n",
    "EMBEDDING_DIM= 256\n",
    "KEY_DIM= 256\n",
    "N_HEADS= 5\n",
    "DROPOUT_RATE= 0.3\n",
    "FEED_FORWARD_DIM= 256\n",
    "LOAD_MODEL= False\n",
    "\n",
    "# Optimization\n",
    "EPOCHS= 5000\n",
    "BATCH_SIZE= 256\n",
    "GENERATE_LEN= 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac76a072-1f34-4c7c-a819-c33b003d9975",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "959e9582-cd8c-457d-90c7-537bb1e4da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_midi_files(\n",
    "    file_list: List[str],\n",
    "    parser: music21.converter.Converter,\n",
    "    seq_len: int,\n",
    "    parsed_data_path: Optional[str] = None\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Parses a list of MIDI files and extracts note sequences and their corresponding durations.\n",
    "\n",
    "    Args:\n",
    "        file_list (List[str]): A list of file paths to MIDI files to be parsed.\n",
    "        parser (music21.converter.Converter): A music21 parser object to read and process the MIDI files.\n",
    "        seq_len (int): The length of the note and duration sequences to be generated.\n",
    "        parsed_data_path (Optional[str]): Path to save the parsed note and duration sequences (if provided).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: A tuple containing two lists:\n",
    "            - notes_list: A list of note sequences of length `seq_len`.\n",
    "            - duration_list: A list of corresponding duration sequences of length `seq_len`.\n",
    "    \"\"\"\n",
    "    notes_list = []  # List to store sequences of notes\n",
    "    duration_list = []  # List to store sequences of durations\n",
    "    notes = []  # Temporary list to hold notes for a single file\n",
    "    durations = []  # Temporary list to hold durations for a single file\n",
    "\n",
    "    # Loop through each MIDI file in the file list\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i + 1, f'Parsing {file}')\n",
    "        score = parser.parse(file).chordify()  # Convert the score to chords\n",
    "\n",
    "        # Add a start token to the sequence\n",
    "        notes.append('START')\n",
    "        durations.append('0.0')\n",
    "\n",
    "        # Iterate over the flattened elements of the score\n",
    "        for element in score.flat:\n",
    "            note_name = None\n",
    "            duration_name = None\n",
    "\n",
    "            # Determine the type of musical element and extract the note and duration\n",
    "            if isinstance(element, music21.key.Key):\n",
    "                note_name = f\"{element.tonic.name}:{element.mode}\"  # Extract key signature\n",
    "                duration_name = '0.0'\n",
    "            elif isinstance(element, music21.meter.TimeSignature):\n",
    "                note_name = f\"{element.ratioString}TS\"  # Extract time signature\n",
    "                duration_name = '0.0'\n",
    "            elif isinstance(element, music21.chord.Chord):\n",
    "                note_name = element.pitches[-1].nameWithOctave  # Use the highest pitch in the chord\n",
    "                duration_name = str(element.duration.quarterLength)\n",
    "            elif isinstance(element, music21.note.Rest):\n",
    "                note_name = str(element.name)  # Extract rest\n",
    "                duration_name = str(element.duration.quarterLength)\n",
    "            elif isinstance(element, music21.note.Note):\n",
    "                note_name = element.nameWithOctave  # Extract note with octave\n",
    "                duration_name = str(element.duration.quarterLength)\n",
    "\n",
    "            # Append note and duration if both were successfully extracted\n",
    "            if note_name and duration_name:\n",
    "                notes.append(note_name)\n",
    "                durations.append(duration_name)\n",
    "\n",
    "        print(f'{len(notes)} notes parsed')  # Log the number of parsed notes\n",
    "\n",
    "    # Generate sequences of notes and durations of length `seq_len`\n",
    "    print(f'Building sequences of length {seq_len}')\n",
    "    for i in range(len(notes) - seq_len):\n",
    "        notes_list.append(' '.join(notes[i: i + seq_len]))\n",
    "        duration_list.append(' '.join(durations[i: i + seq_len]))\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(parsed_data_path, exist_ok=True)\n",
    "\n",
    "    # Save the parsed sequences to files if `parsed_data_path` is provided\n",
    "    if parsed_data_path:\n",
    "        with open(os.path.join(parsed_data_path, 'notes.pkl'), 'wb') as f:\n",
    "            pkl.dump(notes_list, f)\n",
    "        with open(os.path.join(parsed_data_path, 'durations.pkl'), 'wb') as f:\n",
    "            pkl.dump(duration_list, f)\n",
    "\n",
    "    return notes_list, duration_list\n",
    "\n",
    "def load_parsed_files(parsed_data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads the parsed note and duration sequences from pickle files.\n",
    "\n",
    "    Args:\n",
    "        parsed_data_path (str): The directory path where the parsed files ('notes' and 'durations') are stored.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: A tuple containing:\n",
    "            - notes (List[str]): A list of note sequences.\n",
    "            - durations (List[str]): A list of corresponding duration sequences.\n",
    "    \"\"\"\n",
    "    # Load the note sequences from the pickle file\n",
    "    with open(os.path.join(parsed_data_path, 'notes'), 'rb') as f:\n",
    "        notes = pkl.load(f)\n",
    "\n",
    "    # Load the duration sequences from the pickle file\n",
    "    with open(os.path.join(parsed_data_path, 'durations'), 'rb') as f:\n",
    "        durations = pkl.load(f)\n",
    "\n",
    "    return notes, durations\n",
    "\n",
    "def get_midi_notes(sample_note: str, sample_duration: str) -> music21.note.NotRest:\n",
    "    \"\"\"\n",
    "    Converts a sample note and its duration into a corresponding Music21 note, chord, rest, or key signature object.\n",
    "\n",
    "    Args:\n",
    "        sample_note (str): The name of the note or musical element (e.g., 'C4', 'rest', 'C4.E4.G4' for a chord, or '4/4TS').\n",
    "        sample_duration (str): The duration of the note in terms of quarter length, represented as a string (e.g., '0.5', '1', '1.5').\n",
    "\n",
    "    Returns:\n",
    "        new_note (music21.note.Note, music21.chord.Chord, music21.key.Key, or music21.meter.TimeSignature): \n",
    "        A Music21 object corresponding to the provided note and duration.\n",
    "    \"\"\"\n",
    "    new_note = None\n",
    "\n",
    "    # Handle Time Signature (e.g., '4/4TS')\n",
    "    if 'TS' in sample_note:\n",
    "        new_note = music21.meter.TimeSignature(sample_note.split('TS')[0])\n",
    "\n",
    "    # Handle Key Signature (e.g., 'C:major', 'A:minor')\n",
    "    elif 'major' in sample_note or 'minor' in sample_note:\n",
    "        tonic, mode = sample_note.split(':')\n",
    "        new_note = music21.key.Key(tonic, mode)\n",
    "\n",
    "    # Handle Rest\n",
    "    elif sample_note == 'rest':\n",
    "        new_note = music21.note.Rest()\n",
    "        new_note.duration = music21.duration.Duration(float(Fraction(sample_duration)))\n",
    "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
    "\n",
    "    # Handle Chord (e.g., 'C4.E4.G4')\n",
    "    elif '.' in sample_note:\n",
    "        notes_in_chord = sample_note.split('.')\n",
    "        chord_notes = []\n",
    "\n",
    "        for current_note in notes_in_chord:\n",
    "            n = music21.note.Note(current_note)\n",
    "            n.duration = music21.duration.Duration(float(Fraction(sample_duration)))\n",
    "            n.storedInstrument = music21.instrument.Violoncello()\n",
    "            chord_notes.append(n)\n",
    "\n",
    "        new_note = music21.chord.Chord(chord_notes)\n",
    "\n",
    "    # Handle Single Note (e.g., 'C4')\n",
    "    elif sample_note != 'START':\n",
    "        new_note = music21.note.Note(sample_note)\n",
    "        new_note.duration = music21.duration.Duration(float(Fraction(sample_duration)))\n",
    "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
    "\n",
    "    return new_note\n",
    "\n",
    "def create_dataset(elements: List[str]) -> Tuple[tf.data.Dataset, layers.TextVectorization, List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from the input elements, batches and shuffles it,\n",
    "    and applies a TextVectorization layer to convert text data into integer sequences.\n",
    "\n",
    "    Args:\n",
    "        elements (List[str]): A list of text data. Each element in the list represents a text sample.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.data.Dataset, layers.TextVectorization, List[str]]: A tuple containing:\n",
    "            - ds (tf.data.Dataset): A batched and shuffled TensorFlow dataset.\n",
    "            - vectorize_layer (layers.TextVectorization): A TextVectorization layer fitted to the dataset.\n",
    "            - vocab (List[str]): The vocabulary extracted from the TextVectorization layer.\n",
    "    \n",
    "    Example:\n",
    "        elements = [\"hello world\", \"this is a test\", \"deep learning with tensorflow\"]\n",
    "        ds, vectorize_layer, vocab = create_dataset(elements)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input elements into a TensorFlow dataset\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(elements)  # Create dataset slices from input list\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)       # Batch the dataset with specified batch size, dropping the remainder\n",
    "        .shuffle(1000)                                # Shuffle the dataset with a buffer size of 1000\n",
    "    )\n",
    "\n",
    "    # Initialize a TextVectorization layer to convert text into sequences of integers\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=None,    # No text preprocessing or standardization\n",
    "        output_mode='int'    # Output mode is integer indices for each token\n",
    "    )\n",
    "\n",
    "    # Fit the TextVectorization layer to the dataset to build the vocabulary\n",
    "    vectorize_layer.adapt(ds)\n",
    "\n",
    "    # Extract the vocabulary list from the TextVectorization layer\n",
    "    vocab = vectorize_layer.get_vocabulary()  # Vocabulary is a list of words or tokens\n",
    "    \n",
    "    # Return the dataset, the fitted TextVectorization layer, and the vocabulary\n",
    "    return ds, vectorize_layer, vocab\n",
    "\n",
    "def prepare_inputs(notes: tf.Tensor, durations: tf.Tensor) -> Tuple[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Tensor, tf.Tensor]]:\n",
    "    \"\"\"\n",
    "    Prepares input and target sequences for training by tokenizing notes and durations.\n",
    "\n",
    "    Args:\n",
    "        notes (tf.Tensor): A tensor of musical notes with shape (batch_size, sequence_length).\n",
    "        durations (tf.Tensor): A tensor of note durations with shape (batch_size, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Tensor, tf.Tensor]]: A tuple containing:\n",
    "            - x (Tuple[tf.Tensor, tf.Tensor]): The input tensors (tokenized notes and durations) with the last token removed.\n",
    "            - y (Tuple[tf.Tensor, tf.Tensor]): The target tensors (tokenized notes and durations) with the first token removed.\n",
    "\n",
    "    Example:\n",
    "        Suppose `notes` is a tensor of shape (batch_size, sequence_length) representing sequences of musical notes,\n",
    "        and `durations` is a tensor of the same shape representing note durations. This function tokenizes them and\n",
    "        prepares the data for input-output pairs to be used in a sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expand the dimensions of notes and durations to match the expected input shape for vectorization.\n",
    "    # Adds an extra dimension at the end, making the shape (batch_size, sequence_length, 1).\n",
    "    notes = tf.expand_dims(notes, -1)\n",
    "    durations = tf.expand_dims(durations, -1)\n",
    "\n",
    "    # Tokenize the expanded note sequences using a predefined vectorization layer.\n",
    "    tokenized_notes = notes_vectorize_layer(notes)\n",
    "\n",
    "    # Tokenize the expanded duration sequences using a predefined vectorization layer.\n",
    "    tokenized_durations = durations_vectorize_layer(durations)\n",
    "\n",
    "    # Prepare the input sequences by removing the last token from each sequence.\n",
    "    # `x` is the input tuple: (tokenized_notes[:, :-1], tokenized_durations[:, :-1])\n",
    "    x = (tokenized_notes[:, :-1], tokenized_durations[:, :-1])\n",
    "\n",
    "    # Prepare the target sequences by removing the first token from each sequence.\n",
    "    # `y` is the target tuple: (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
    "    y = (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
    "\n",
    "    # Return the input and target pairs for training a sequence-to-sequence model.\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fff5ea-f987-4215-8203-971c9dbf3265",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1a8b9-ff01-4edc-92f2-cfba0135af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinePositionEncoding(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding layer.\n",
    "\n",
    "    This layer computes positional encodings using a mixture of sine and cosine\n",
    "    functions with geometrically increasing wavelengths, as described in the paper\n",
    "    'Attention Is All You Need' by Vaswani et al. (2017).\n",
    "\n",
    "    Args:\n",
    "        max_wavelength (int): The maximum angular wavelength of the sine and cosine\n",
    "                              functions. Default is 10,000.\n",
    "\n",
    "    Input:\n",
    "        inputs (tf.Tensor): A 3D tensor of shape [batch_size, sequence_length, hidden_size],\n",
    "                            where hidden_size is the feature dimension.\n",
    "\n",
    "    Output:\n",
    "        tf.Tensor: A positional encoding tensor with the same shape as the input.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        seq_len = 100\n",
    "        vocab_size = 1000\n",
    "        embedding_dim = 32\n",
    "\n",
    "        inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
    "        embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "        positional_encoding = SinePositionEncoding()(embedding)\n",
    "        outputs = embedding + positional_encoding\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_wavelength: int = 10000, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the SinePositionEncoding layer.\n",
    "\n",
    "        Args:\n",
    "            max_wavelength (int): The maximum angular wavelength of the sine and cosine\n",
    "                                  curves used for positional encoding.\n",
    "            **kwargs: Additional keyword arguments passed to the base `keras.layers.Layer` class.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = max_wavelength\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the sinusoidal positional encodings for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): A 3D tensor of shape [batch_size, sequence_length, hidden_size].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: A 3D tensor of shape [batch_size, sequence_length, hidden_size]\n",
    "                       containing the sinusoidal positional encodings.\n",
    "        \"\"\"\n",
    "        # Get the input shape dynamically\n",
    "        input_shape = tf.shape(inputs)\n",
    "\n",
    "        # Sequence length is the second-to-last dimension of the input\n",
    "        seq_length = input_shape[-2]\n",
    "        # Hidden size (embedding dimension) is the last dimension of the input\n",
    "        hidden_size = input_shape[-1]\n",
    "\n",
    "        # Create a tensor of positions from 0 to seq_length - 1, cast to the layer's dtype\n",
    "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
    "\n",
    "        # Compute the minimum frequency for the sinusoidal functions\n",
    "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
    "\n",
    "        # Compute the scaling timescales for each hidden dimension\n",
    "        # Shape: [hidden_size]\n",
    "        timescales = tf.pow(\n",
    "            min_freq,\n",
    "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
    "            / tf.cast(hidden_size, self.compute_dtype),\n",
    "        )\n",
    "\n",
    "        # Compute the angles (position * timescales) for sine and cosine functions\n",
    "        # Shape: [seq_length, hidden_size]\n",
    "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
    "\n",
    "        # Create masks for sine (even indices) and cosine (odd indices)\n",
    "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)  # [0, 1, 0, 1, ...]\n",
    "        sin_mask = 1 - cos_mask  # [1, 0, 1, 0, ...]\n",
    "\n",
    "        # Compute positional encodings by applying sine to even indices and cosine to odd indices\n",
    "        # Shape: [seq_length, hidden_size]\n",
    "        positional_encodings = (\n",
    "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
    "        )\n",
    "\n",
    "        # Broadcast the positional encodings to match the input shape\n",
    "        return tf.broadcast_to(positional_encodings, input_shape)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"\n",
    "        Return the configuration of the layer for serialization.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the layer's configuration.\n",
    "        \"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\"max_wavelength\": self.max_wavelength})\n",
    "        return config\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b06a3-b04b-462f-bcd0-e42c5cc45fd4",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc88ad9-f830-4b2f-a744-5a2426b07b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "output_dir = \"data/bach-cello\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of URLs to download\n",
    "urls = [\n",
    "    \"http://www.jsbach.net/midi/cs1-1pre.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs1-2all.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs1-3cou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs1-4sar.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs1-5men.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs1-6gig.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs2-1pre.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs2-2all.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs2-3cou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs2-4sar.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs2-5men.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs2-6gig.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs3-1pre.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs3-2all.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs3-3cou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs3-4sar.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs3-5bou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs3-6gig.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs4-1pre.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs4-2all.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs4-3cou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs4-4sar.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs4-5bou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs4-6gig.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs5-1pre.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs5-2all.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs5-3cou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs5-4sar.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs5-5gav.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs5-6gig.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs6-1pre.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs6-2all.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs6-3cou.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs6-4sar.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs6-5gav.mid\",\n",
    "    \"http://www.jsbach.net/midi/cs6-6gig.mid\"\n",
    "]\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, output_dir):\n",
    "    file_name = os.path.join(output_dir, os.path.basename(url))\n",
    "    print(f\"Downloading {file_name}...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"✅ Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to download: {file_name} (Status: {response.status_code})\")\n",
    "\n",
    "# Download each file in the list\n",
    "#for url in urls:\n",
    "#    download_file(url, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b720bf8e-699a-40b6-ad16-d2de41a86db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list= glob.glob(DATA_PATH + '*.mid')\n",
    "parser= music21.converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10bebc00-6de1-4c67-b5c5-a169a862b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_score = (\n",
    "    music21.converter.parse(file_list[1]).splitAtQuarterLength(12)[0].chordify()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c90c8d7-869c-43d9-8f67-afba0fc51bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0} <music21.metadata.Metadata object at 0x7fe076878150>\n",
      "{0.0} <music21.stream.Measure 1 offset=0.0>\n",
      "    {0.0} <music21.instrument.Violoncello 'Solo Cello: Solo Cello'>\n",
      "    {0.0} <music21.instrument.Violoncello 'Violoncello'>\n",
      "    {0.0} <music21.clef.BassClef>\n",
      "    {0.0} <music21.tempo.MetronomeMark Quarter=250>\n",
      "    {0.0} <music21.key.Key of G major>\n",
      "    {0.0} <music21.meter.TimeSignature 4/4>\n",
      "    {0.0} <music21.note.Rest 3.75ql>\n",
      "    {3.5} <music21.tempo.MetronomeMark Quarter=77>\n",
      "    {3.75} <music21.chord.Chord B3>\n",
      "{4.0} <music21.stream.Measure 2 offset=4.0>\n",
      "    {0.0} <music21.chord.Chord G2 D3 B3>\n",
      "    {1.0} <music21.chord.Chord B3>\n",
      "    {1.25} <music21.chord.Chord A3>\n",
      "    {1.5} <music21.chord.Chord G3>\n",
      "    {1.75} <music21.chord.Chord F#3>\n",
      "    {2.0} <music21.chord.Chord G3>\n",
      "    {2.25} <music21.chord.Chord D3>\n",
      "    {2.5} <music21.chord.Chord E3>\n",
      "    {2.75} <music21.chord.Chord F#3>\n",
      "    {3.0} <music21.chord.Chord G3>\n",
      "    {3.25} <music21.chord.Chord A3>\n",
      "    {3.5} <music21.chord.Chord B3>\n",
      "    {3.75} <music21.chord.Chord C4>\n",
      "{8.0} <music21.stream.Measure 3 offset=8.0>\n",
      "    {0.0} <music21.chord.Chord D4>\n",
      "    {0.25} <music21.chord.Chord B3>\n",
      "    {0.5} <music21.chord.Chord G3>\n",
      "    {0.75} <music21.chord.Chord F#3>\n",
      "    {1.0} <music21.chord.Chord G3>\n",
      "    {1.25} <music21.chord.Chord E3>\n",
      "    {1.5} <music21.chord.Chord D3>\n",
      "    {1.75} <music21.chord.Chord C3>\n",
      "    {2.0} <music21.chord.Chord B2>\n",
      "    {2.25} <music21.chord.Chord C3>\n",
      "    {2.5} <music21.chord.Chord D3>\n",
      "    {2.75} <music21.chord.Chord E3>\n",
      "    {3.0} <music21.chord.Chord F#3>\n",
      "    {3.25} <music21.chord.Chord G3>\n",
      "    {3.5} <music21.chord.Chord A3>\n",
      "    {3.75} <music21.chord.Chord B3>\n"
     ]
    }
   ],
   "source": [
    "example_score.show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8629cea7-c062-4686-a5ca-97bf425a62bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Parsing ./datasets/bach-cello/cs1-1pre.mid\n",
      "658 notes parsed\n",
      "2 Parsing ./datasets/bach-cello/cs1-2all.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32011/1394043709.py:2: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  notes, durations= parse_midi_files(file_list, parser, SEQ_LEN + 1, PARSED_DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579 notes parsed\n",
      "3 Parsing ./datasets/bach-cello/cs1-3cou.mid\n",
      "2399 notes parsed\n",
      "4 Parsing ./datasets/bach-cello/cs1-4sar.mid\n",
      "2662 notes parsed\n",
      "5 Parsing ./datasets/bach-cello/cs1-5men.mid\n",
      "3309 notes parsed\n",
      "6 Parsing ./datasets/bach-cello/cs1-6gig.mid\n",
      "3735 notes parsed\n",
      "7 Parsing ./datasets/bach-cello/cs2-1pre.mid\n",
      "4373 notes parsed\n",
      "8 Parsing ./datasets/bach-cello/cs2-2all.mid\n",
      "5066 notes parsed\n",
      "9 Parsing ./datasets/bach-cello/cs2-3cou.mid\n",
      "5807 notes parsed\n",
      "10 Parsing ./datasets/bach-cello/cs2-4sar.mid\n",
      "6144 notes parsed\n",
      "11 Parsing ./datasets/bach-cello/cs2-5men.mid\n",
      "6671 notes parsed\n",
      "12 Parsing ./datasets/bach-cello/cs2-6gig.mid\n",
      "7406 notes parsed\n",
      "13 Parsing ./datasets/bach-cello/cs3-1pre.mid\n",
      "8387 notes parsed\n",
      "14 Parsing ./datasets/bach-cello/cs3-2all.mid\n",
      "9124 notes parsed\n",
      "15 Parsing ./datasets/bach-cello/cs3-3cou.mid\n",
      "10113 notes parsed\n",
      "16 Parsing ./datasets/bach-cello/cs3-4sar.mid\n",
      "10454 notes parsed\n",
      "17 Parsing ./datasets/bach-cello/cs3-5bou.mid\n",
      "11335 notes parsed\n",
      "18 Parsing ./datasets/bach-cello/cs3-6gig.mid\n",
      "12296 notes parsed\n",
      "19 Parsing ./datasets/bach-cello/cs4-1pre.mid\n",
      "13113 notes parsed\n",
      "20 Parsing ./datasets/bach-cello/cs4-2all.mid\n",
      "14168 notes parsed\n",
      "21 Parsing ./datasets/bach-cello/cs4-3cou.mid\n",
      "15090 notes parsed\n",
      "22 Parsing ./datasets/bach-cello/cs4-4sar.mid\n",
      "15410 notes parsed\n",
      "23 Parsing ./datasets/bach-cello/cs4-5bou.mid\n",
      "16734 notes parsed\n",
      "24 Parsing ./datasets/bach-cello/cs4-6gig.mid\n",
      "17705 notes parsed\n",
      "25 Parsing ./datasets/bach-cello/cs5-1pre.mid\n",
      "19014 notes parsed\n",
      "26 Parsing ./datasets/bach-cello/cs5-2all.mid\n",
      "19706 notes parsed\n",
      "27 Parsing ./datasets/bach-cello/cs5-3cou.mid\n",
      "20136 notes parsed\n",
      "28 Parsing ./datasets/bach-cello/cs5-4sar.mid\n",
      "20355 notes parsed\n",
      "29 Parsing ./datasets/bach-cello/cs5-5gav.mid\n",
      "21557 notes parsed\n",
      "30 Parsing ./datasets/bach-cello/cs5-6gig.mid\n",
      "22006 notes parsed\n",
      "31 Parsing ./datasets/bach-cello/cs6-1pre.mid\n",
      "23348 notes parsed\n",
      "32 Parsing ./datasets/bach-cello/cs6-2all.mid\n",
      "24033 notes parsed\n",
      "33 Parsing ./datasets/bach-cello/cs6-3cou.mid\n",
      "25319 notes parsed\n",
      "34 Parsing ./datasets/bach-cello/cs6-4sar.mid\n",
      "25654 notes parsed\n",
      "35 Parsing ./datasets/bach-cello/cs6-5gav.mid\n",
      "26460 notes parsed\n",
      "36 Parsing ./datasets/bach-cello/cs6-6gig.mid\n",
      "27632 notes parsed\n",
      "Building sequences of length 51\n"
     ]
    }
   ],
   "source": [
    "if PARSE_MIDI_FILES:\n",
    "    notes, durations= parse_midi_files(file_list, parser, SEQ_LEN + 1, PARSED_DATA_PATH)\n",
    "else:\n",
    "    notes, durations= load_parsed_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0458a8ce-d4fa-451d-b075-4a9f5eac2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notes string\n",
      " START G:major 4/4TS rest B3 B3 B3 A3 G3 F#3 G3 D3 E3 F#3 G3 A3 B3 C4 D4 B3 G3 F#3 G3 E3 D3 C3 B2 C3 D3 E3 F#3 G3 A3 B3 C4 A3 G3 F#3 G3 E3 F#3 G3 A2 D3 F#3 G3 A3 B3 C4 A3 B3 ...\n",
      "\n",
      "Durations string\n",
      " 0.0 0.0 0.0 3.75 0.25 1.0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...\n"
     ]
    }
   ],
   "source": [
    "example_notes= notes[658]\n",
    "example_durations= durations[658]\n",
    "print('\\nNotes string\\n', example_notes, '...')\n",
    "print('\\nDurations string\\n', example_durations, '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d3732-c8db-494b-a8a4-2d2dc1888504",
   "metadata": {},
   "source": [
    "## TOKENIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfcd61d8-df69-44f1-88fd-75472b8918b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:36:10.494056: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-12-05 22:36:11.212523: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "notes_seq_ds, notes_vectorize_layer, notes_vocab= create_dataset(notes)\n",
    "durations_seq_ds, durations_vectorize_layer, durations_vocab= create_dataset(durations)\n",
    "seq_ds= tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6e69910-985a-4454-b737-60c94af6c04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note token duration token\n",
      "        37          9\n",
      "        51          9\n",
      "        42          9\n",
      "        33         18\n",
      "         9          2\n",
      "         9          4\n",
      "         9          2\n",
      "         3          2\n",
      "         2          2\n",
      "        12          2\n",
      "         2          2\n"
     ]
    }
   ],
   "source": [
    "example_tokenised_notes= notes_vectorize_layer(example_notes)\n",
    "example_tokenised_durations= durations_vectorize_layer(example_durations)\n",
    "\n",
    "print('{:10} {:10}'.format('note token', 'duration token'))\n",
    "for i, (note_int, duration_int) in enumerate(\n",
    "    zip(\n",
    "        example_tokenised_notes.numpy()[:11],\n",
    "        example_tokenised_durations.numpy()[:11]\n",
    "    )):\n",
    "    print(f'{note_int:10} {duration_int:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e896a70-ed2e-4024-9069-f7b70251d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTES_VOCAB: length= 59\n",
      "0: \n",
      "1: [UNK]\n",
      "2: G3\n",
      "3: A3\n",
      "4: D3\n",
      "5: F3\n",
      "6: C4\n",
      "7: D4\n",
      "8: E3\n",
      "9: B3\n",
      "\n",
      "DURATIONS_VOCAB: length= 24\n",
      "0: \n",
      "1: [UNK]\n",
      "2: 0.25\n",
      "3: 0.5\n",
      "4: 1.0\n",
      "5: 1/3\n",
      "6: 0.75\n",
      "7: 1/12\n",
      "8: 1.5\n",
      "9: 0.0\n"
     ]
    }
   ],
   "source": [
    "notes_vocab_size= len(notes_vocab)\n",
    "durations_vocab_size= len(durations_vocab)\n",
    "\n",
    "print(f'\\nNOTES_VOCAB: length= {len(notes_vocab)}')\n",
    "\n",
    "for i, note in enumerate(notes_vocab[:10]):\n",
    "    print(f'{i}: {note}')\n",
    "\n",
    "print(f'\\nDURATIONS_VOCAB: length= {len(durations_vocab)}')\n",
    "\n",
    "for i, note in enumerate(durations_vocab[:10]):\n",
    "    print(f'{i}: {note}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c32c3-8989-49d2-b9ca-6c52ff951ef5",
   "metadata": {},
   "source": [
    "## TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03ee851b-11d1-422d-843b-52d14ce0d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_ds.map(prepare_inputs):\n",
    "# Converts the original sequential dataset (seq_ds) into (input, target) pairs for each batch.\n",
    "\n",
    "# .repeat(DATASET_REPETITIONS):\n",
    "# Repeats the entire dataset DATASET_REPETITIONS times to provide multiple epochs worth of training data.\n",
    "\n",
    "ds= seq_ds.map(prepare_inputs).repeat(DATASET_REPETTIOTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e99962f4-d18c-4734-bdf7-d8a297809e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(256, 50), dtype=int64, numpy=\n",
      "array([[ 2,  5, 31, ..., 14, 11,  2],\n",
      "       [ 5, 31,  2, ..., 11,  2, 10],\n",
      "       [31,  2,  5, ...,  2, 10,  4],\n",
      "       ...,\n",
      "       [11, 14,  2, ...,  6, 16,  6],\n",
      "       [14,  2,  5, ..., 16,  6, 11],\n",
      "       [ 2,  5,  2, ...,  6, 11, 14]])>, <tf.Tensor: shape=(256, 50), dtype=int64, numpy=\n",
      "array([[3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       ...,\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3]])>), (<tf.Tensor: shape=(256, 50), dtype=int64, numpy=\n",
      "array([[ 5, 31,  2, ..., 11,  2, 10],\n",
      "       [31,  2,  5, ...,  2, 10,  4],\n",
      "       [ 2,  5, 10, ..., 10,  4, 13],\n",
      "       ...,\n",
      "       [14,  2,  5, ..., 16,  6, 11],\n",
      "       [ 2,  5,  2, ...,  6, 11, 14],\n",
      "       [ 5,  2, 11, ..., 11, 14,  2]])>, <tf.Tensor: shape=(256, 50), dtype=int64, numpy=\n",
      "array([[3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       ...,\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3],\n",
      "       [3, 3, 3, ..., 3, 3, 3]])>))\n"
     ]
    }
   ],
   "source": [
    "example_input_output= ds.take(1).get_single_element()\n",
    "print(example_input_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926677d-df39-4314-8530-b81aa93acf60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
