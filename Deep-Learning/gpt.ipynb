{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d9e4fa-4537-406c-89f7-dbf7e03ca7c2",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600ea52-768a-4bac-a788-53b311cf55f5",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc942bb-64bc-4acf-870b-65cebad04a4c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514987ff-ab57-41bd-9429-7efd0e048530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 07:29:43.763224: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 07:29:43.790135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 07:29:43.790178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 07:29:43.791400: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 07:29:43.796346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    losses,\n",
    "    callbacks\n",
    ")\n",
    "import numpy as np\n",
    "import datetime\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db07681-d157-4b6a-a4e6-b615842891ae",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a050a5ca-5daf-41b1-941d-85fdb3a818f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(s):\n",
    "    \"\"\"\n",
    "    Adds spaces around punctuation symbols in a string and normalizes whitespace.\n",
    "\n",
    "    This function takes an input string, identifies all punctuation characters,\n",
    "    and surrounds them with spaces. It then removes any instances of multiple\n",
    "    consecutive spaces, ensuring the string is neatly formatted.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    s : str\n",
    "        The input string to process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The processed string with spaces around punctuation and normalized whitespace.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> pad_punctuation(\"Hello,world! This is a test.\")\n",
    "    'Hello , world ! This is a test .'\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function uses Python's `string.punctuation` to identify all standard punctuation symbols.\n",
    "    - Regular expressions are used to handle the replacement efficiently.\n",
    "    \"\"\"\n",
    "    \n",
    "    # buld a puctuation symbol regular expression, and then replace coughted puctuation symbol with ' symbol '\n",
    "    s= re.sub(f'([{string.punctuation}])', r' \\1 ', s)\n",
    "    # replace all occurance of more the 1 whitespace in the row\n",
    "    return re.sub(' +', ' ', s)   \n",
    "\n",
    "def prepare_inputs(text):\n",
    "    \"\"\"\n",
    "    Prepares input and target tensors for a token prediction model.\n",
    "\n",
    "    This transformation processes a batch of text data to help the model learn to predict \n",
    "    the next token in a sentence by analyzing previous tokens within the same sentence.\n",
    "\n",
    "    Args:\n",
    "        text (tf.Tensor): A batch of raw text input, typically as a 1D tensor or list of strings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (x, y) where:\n",
    "            - x (tf.Tensor): The input tensor containing all tokens of each sentence \n",
    "              except the last token, with shape (batch_size, sequence_length-1).\n",
    "            - y (tf.Tensor): The target tensor containing all tokens of each sentence \n",
    "              starting from the second token, with shape (batch_size, sequence_length-1).\n",
    "    \n",
    "    Steps:\n",
    "        1. Expands the text tensor by adding a new dimension at the end.\n",
    "        2. Tokenizes the sentences using the provided `vectorize_layer`.\n",
    "        3. Splits the tokenized sentences into:\n",
    "            - `x`: All tokens except the last one.\n",
    "            - `y`: All tokens except the first one.\n",
    "    \"\"\"\n",
    "    # Add 1 dimension to the text data for compatibility with the vectorizer\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    \n",
    "    # Tokenize sentences using the vectorize_layer (assumed to be predefined)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "\n",
    "    # Input tensor: all tokens except the last one\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    \n",
    "    # Target tensor: all tokens starting from the second one\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Generates a causal attention mask for Transformer-based models.\n",
    "\n",
    "    A causal mask ensures that for each destination (decoder) token, the model can only\n",
    "    attend to source (encoder) tokens that have already been processed or align in time.\n",
    "    This is critical for tasks such as autoregressive generation where future tokens should\n",
    "    not be visible.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences in the batch.\n",
    "        n_dest (int): Number of destination tokens (e.g., in the decoder).\n",
    "        n_src (int): Number of source tokens (e.g., in the encoder or the decoder itself).\n",
    "        dtype (tf.DType): Data type for the resulting mask (e.g., `tf.float32`).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A mask tensor of shape `(batch_size, n_dest, n_src)` with values of 1.0\n",
    "                   for allowed positions and 0.0 for disallowed positions.\n",
    "    \"\"\"\n",
    "    # Create a range of indices for destination tokens and reshape to a column vector\n",
    "    i = tf.range(n_dest)[:, None]  # Shape: (n_dest, 1)\n",
    "    \n",
    "    # Create a range of indices for source tokens\n",
    "    j = tf.range(n_src)  # Shape: (n_src,)\n",
    "    \n",
    "    # Compare indices to establish causality: i >= j - offset\n",
    "    # This ensures the mask allows only past and current positions\n",
    "    # j - n_src + n_dest - computes an adjusted offset for the indices of the source tokens (j) to align them with the destination tokens (i).\n",
    "    # i >= (adjusted j) - 1. it is where the causality is enforced. The mask sets True when a destination token's index i is greater \n",
    "    #                     than or equal to the adjusted source token index j - n_src + n_dest.\n",
    "    #                     2. it  ensures that a destination token can \"see\" only the tokens in the source that come before it or align with it.\n",
    "    m = i >= j - n_src + n_dest  # Shape: (n_dest, n_src)\n",
    "    \n",
    "    # Cast the boolean mask into the specified dtype (e.g., float32 or float16)\n",
    "    mask = tf.cast(m, dtype)  # Shape: (n_dest, n_src)\n",
    "    \n",
    "    # Reshape the mask to add a singleton batch dimension\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])  # Shape: (1, n_dest, n_src)\n",
    "    \n",
    "    # Calculate the multiplier for tiling the mask to match batch size\n",
    "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
    "    # `mult` is a vector specifying how many times to replicate along each axis\n",
    "    \n",
    "    # Tile the mask along the batch dimension\n",
    "    return tf.tile(mask, mult)  # Final shape: (batch_size, n_dest, n_src)\n",
    "\n",
    "def print_probs(info, vocab, top_k=5):\n",
    "    \"\"\"\n",
    "    Prints the top-k predicted probabilities of words along with their attention scores\n",
    "    visualized as HTML for a given prompt and model output.\n",
    "\n",
    "    Args:\n",
    "        info (list of dicts): A list of dictionaries containing:\n",
    "            - 'prompt' (str): The input prompt sentence for which probabilities are being analyzed.\n",
    "            - 'atts' (np.ndarray): Attention scores corresponding to the words in the prompt.\n",
    "            - 'word_probs' (np.ndarray): Probabilities associated with each word in the vocabulary.\n",
    "        vocab (list of str): A list of vocabulary words indexed to match the probabilities.\n",
    "        top_k (int, optional): The number of top probabilities to display for each prompt. Defaults to 5.\n",
    "\n",
    "    Output:\n",
    "        Displays HTML to highlight words in the prompt based on their attention scores.\n",
    "        Prints the top-k most probable words from the vocabulary along with their probabilities.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop through each element in the info list.\n",
    "    for i in info:\n",
    "        highlighted_text = []  # Initialize an empty list to store HTML strings for highlighted words.\n",
    "\n",
    "        # Loop through each word in the prompt and its corresponding average attention score.\n",
    "        for word, att_score in zip(i['prompt'].split(), np.mean(i['atts'], axis=0)):\n",
    "            # Create a span element with a background color that varies based on the attention score.\n",
    "            highlighted_text.append(\n",
    "                '<span style=\"background-color:rgba(135,206,250,'\n",
    "                + str(att_score / max(np.mean(i[\"atts\"], axis=0)))  # Normalize attention score.\n",
    "                + ');\">'\n",
    "                + word\n",
    "                + \"</span>\"\n",
    "            )\n",
    "        \n",
    "        # Join the list of highlighted words into a single string.\n",
    "        highlighted_text = ' '.join(highlighted_text)\n",
    "        \n",
    "        # Display the HTML to visualize the attention scores as background color.\n",
    "        display(HTML(highlighted_text))\n",
    "\n",
    "        # Extract word probabilities for the current prompt.\n",
    "        word_probs = i['word_probs']\n",
    "        \n",
    "        # Sort the word probabilities in descending order and take the top-k.\n",
    "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
    "        \n",
    "        # Get the indices of the top-k probabilities.\n",
    "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
    "\n",
    "        # Print the top-k words along with their probabilities (rounded to 2 decimal places).\n",
    "        for p, i in zip(p_sorted, i_sorted):\n",
    "            print(f'{vocab[i]}:    \\t{np.round(100 * p, 2)}%')\n",
    "\n",
    "        print('-----------\\n')  # Print a separator between different prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762c44c-40ab-455a-b2aa-1165f491ab2d",
   "metadata": {},
   "source": [
    "## PREPARE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305593d-e66e-4d26-8693-6bad88cd2eb4",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdde2842-336a-424a-923d-97fe65352d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6e370-48a3-4115-a409-65284cb782b7",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f173f7-3700-4fb5-a516-4d1f5c003ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/wine-reviews/winemag-data-130k-v2.json') as json_data:\n",
    "    wine_data= json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7352e7-6643-4eed-b068-a0a0d011f957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'points': '87',\n",
       " 'title': 'Kirkland Signature 2011 Mountain Cuvée Cabernet Sauvignon (Napa Valley)',\n",
       " 'description': 'Soft, supple plum envelopes an oaky structure in this Cabernet, supported by 15% Merlot. Coffee and chocolate complete the picture, finishing strong at the end, resulting in a value-priced wine of attractive flavor and immediate accessibility.',\n",
       " 'taster_name': 'Virginie Boone',\n",
       " 'taster_twitter_handle': '@vboone',\n",
       " 'price': 19,\n",
       " 'designation': 'Mountain Cuvée',\n",
       " 'variety': 'Cabernet Sauvignon',\n",
       " 'region_1': 'Napa Valley',\n",
       " 'region_2': 'Napa',\n",
       " 'province': 'California',\n",
       " 'country': 'US',\n",
       " 'winery': 'Kirkland Signature'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff43c97-7d50-4d52-9c40-31038d0a7c4e",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84327d16-c973-4729-9a80-f63f29d8212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets\n",
    "filtered_data= [\n",
    "    'wine_review : ' + x['country'] + ' : ' + x['province'] + ' : ' + x['variety'] + ' : ' + x['description'] for x in wine_data \n",
    "    if x['country'] is not None\n",
    "       and x['province'] is not None\n",
    "       and x['variety'] is not None\n",
    "       and x['description'] is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef73e9f-42a7-40d9-893b-01e2274db65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine_review : US : California : Cabernet Sauvignon : Soft, supple plum envelopes an oaky structure in this Cabernet, supported by 15% Merlot. Coffee and chocolate complete the picture, finishing strong at the end, resulting in a value-priced wine of attractive flavor and immediate accessibility.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadca3d9-71c6-4a12-ac20-ac3cf453d907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129907 recepies loaded\n"
     ]
    }
   ],
   "source": [
    "n_wines= len(filtered_data)\n",
    "print(f'{n_wines} recepies loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fca18-76c3-4d30-a74f-4d0751b57422",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43c305-017c-41b7-a5fc-6e302557d10d",
   "metadata": {},
   "source": [
    "### Pad the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e57e1f-f804-4e61-8353-2be5683d0208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.42 s, sys: 27.4 ms, total: 3.45 s\n",
      "Wall time: 3.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~3s\n",
    "text_data= [pad_punctuation(x) for x in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e411dad5-b63e-4ff0-8a08-1e3bc0e0a679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine _ review : US : California : Pinot Noir : Oak and earth intermingle around robust aromas of wet forest floor in this vineyard - designated Pinot that hails from a high - elevation site . Small in production , it offers intense , full - bodied raspberry and blackberry steeped in smoky spice and smooth texture . '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data= text_data[25]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3560ef-4889-4dee-967e-80da846aaabd",
   "metadata": {},
   "source": [
    "### Convert to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81edd78b-aa76-4f04-99a9-00cd3482a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 07:29:52.021802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.063362: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.063414: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.068079: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.068128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.068151: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.153803: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.153864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.153872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-28 07:29:52.153905: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 07:29:52.153926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2245 MB memory:  -> device: 0, name: NVIDIA T1200 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Convert data to a TensorFlow dataset devided by batches with 32 recepies and shuffle buffer thus all recepies are devided randomly\n",
    "text_ds= (\n",
    "    tf.data.Dataset.from_tensor_slices(text_data).batch(32).shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b386723-e206-40f0-8f2c-dd23ac75c8f5",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c2457-1b36-4175-a764-870bba96a574",
   "metadata": {},
   "source": [
    "#### Create Vect. layer\n",
    "\n",
    "Create a Keras TextVectorization layer:\n",
    "- convert text to lowercase\n",
    "- give most prevalent 10k words a corresponding integer token\n",
    "- pad the sequnce to 81 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc9ef583-29ec-4b3c-9de2-e868fd67cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=10000\n",
    "MAX_LEN=80\n",
    "\n",
    "vectorize_layer= layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=80 + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2a306-67fd-4776-a349-7b48dd677c6f",
   "metadata": {},
   "source": [
    "#### Calc text statistics\n",
    "\n",
    "- Apply TextVectorization to the training data\n",
    "- Get the vocabulary of 10k most pevalent words.  \n",
    "  NOTE:  \n",
    "  - all words over 10k will be coded as 1 (i.e. UNK)\n",
    "  - if number of words in sentence less then 201, thouse will be coded as 0 (i.e. stop token - text string come to an end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c94748-ecbc-4026-b160-07c082eee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 2.86 s, total: 17.4 s\n",
      "Wall time: 17.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 07:30:09.583244: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~20s \n",
    "# Adapt layer to the training set\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab=vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96806a46-2486-4460-9fd5-f6e9a7bbf7a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : \n",
      "1 : [UNK]\n",
      "2 : :\n",
      "3 : ,\n",
      "4 : .\n",
      "5 : and\n",
      "6 : the\n",
      "7 : wine\n",
      "8 : a\n",
      "9 : of\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(vocab[:10]):\n",
    "    print(f'{i} : {word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432c9d4-32f7-4ec3-80c8-274781f38cba",
   "metadata": {},
   "source": [
    "INTERIM CONCLUSION\n",
    "\n",
    "We see a subset of tokens mapped to their respctive indices. The layer reserves the 0 token for padding, and 1 for unknown. NOTE:\n",
    "\n",
    "    The other words are assigned tokens in order of frequency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f857e1-756c-4c80-a97e-4894d45dc337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine _ review : US : Oregon : Pinot Gris : Tart and snappy , the flavors of lime flesh and rind dominate . Some green pineapple pokes through , with crisp acidity underscoring the flavors . The wine was all stainless - steel fermented . '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e888286-bb70-4a8a-a7ee-9e59a0442bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   7   11   10    2   21    2  151    2   44  411    2  139    5 1009\n",
      "    3    6   17    9  150 1030    5  681  627    4  105   95  235 6405\n",
      "   85    3   12   74   31 5782    6   17    4    6    7  440  128  879\n",
      "   15  797  542    4    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# display same as above but as converted to int word mappings\n",
    "example_tokenised=vectorize_layer(text_data[2])\n",
    "print(example_tokenised.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd077b8-11a6-4ad4-82b5-75a8cffff654",
   "metadata": {},
   "source": [
    "#### Create Training Dataset\n",
    "\n",
    "prepare_inputs convert the dataset to the MapDataset where each sentnce is splited on to the 2 sets of sequences:\n",
    "- x: contains all words in sentece except last\n",
    "- y: shifted by one left, thus it starts from the 2nd element\n",
    "\n",
    "Thus we will have a tuple `[x,y]` thus when our model will train it will learn relation ships between words as it nos that word `x` the target will be `y`. For example in sentence `The cloud is white` model will learn that `x=The` and `y=cloud` so it will adjust it weight accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56597fbd-87c1-4f00-b09a-d65619db3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds= text_ds.map(prepare_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc5b18a-e79e-4bcc-9d36-f36694d4760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([  7,  11,  10,   2,  43,   2,  66,   2,  66,  15,  64,  25,  28,\n",
       "         2, 219,   3, 103,  35, 627,  13, 171,   7,   4, 106, 127, 169,\n",
       "        84,   3, 430,   8,  90,  59, 792,   4,  16,  19,  22, 140,  58,\n",
       "         5,  65,  18,   6, 134,   3, 434,  12,  39,  38,  68,   5,  60,\n",
       "        17,   4,  16,  19,  22, 694,  20, 161,  72,   5, 688,   4,  36,\n",
       "        33, 292,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_output= train_ds.take(1).get_single_element()\n",
    "example_input_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65cc671c-2b50-4e7e-9e6a-643e7582148e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([   7,   11,   10,    2,   43,    2,   66,    2,   66,   15,   64,\n",
       "         25,   28,    2,  106,  127,   84,  848,   18,   13,  308,    3,\n",
       "        101,   15,  678,    7,    4,   16,   14,  171,    5,  149,   90,\n",
       "        181,    5,  138,   89,   13,  728,    4,    6, 1326,  608,    9,\n",
       "          6,  143,   42,  679,    8,  848,  137,  111,   20,    6,    7,\n",
       "          4,   26,    3,  170,   12,    6,   35,    3,  266,  311,  653,\n",
       "         20,    6,   38,  133,   71,    4,   36,   33,  292,    4,    0,\n",
       "          0,    0,    0])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_output[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c62c6a-d7c0-4120-a3ce-f4f303004354",
   "metadata": {},
   "source": [
    "## BUILD MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ed590-ca80-47cb-a4e6-c765b37c6e83",
   "metadata": {},
   "source": [
    "### Causal attention mask\n",
    "\n",
    "<img src=\"./images/causal-masking.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "\n",
    "**The attention score matrix for a sequence of words**\n",
    "\n",
    "- The green blocks at the top represent the queries — tokens for which the model is trying to predict the next token.\n",
    "- The red blocks on the left represent the keys — tokens that the model searches for relevant information.\n",
    "\n",
    "**Explanation of Causal Masking**\n",
    "- The gray area indicates where the causal mask is applied.\n",
    "- The purpose of the mask is to prevent \"information leakage\" from future words. Without this mask, our GPT model would be able to perfectly guess the next word in the sentence, because it would be using the key from the word itself as a feature. For example, when calculating attention for the word \"the\", the model can only use information from previous words, not the ones that come after.\n",
    "- The mask sets the attention weight for future words to 0. This explains why part of the matrix is gray for the first words like \"the,\" \"pink,\" and \"elephant.\"\n",
    "- Each word's vector is multiplied by the corresponding keys, but the attention to future tokens is blocked by the mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab1a0db-22cd-4efe-b123-2333d523b7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(causal_attention_mask(1,10,10, dtype=tf.int32)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a35e74-980c-42d2-87ca-50a111f3db6a",
   "metadata": {},
   "source": [
    "### Transformer Block Layer\n",
    "\n",
    "<img src=\"./images/transformer-block.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "A Transformer block is a single component within a Transformer that applies some skip connections, feed-forward (dense) layers, and normalization around the multihead attention layer.\n",
    "\n",
    "1. query is passed around the multihead attention layer to be added\n",
    "to the output—this is a skip connection and is common in modern deep learning\n",
    "architectures. It means we can build very deep neural networks that do not suffer as\n",
    "much from the vanishing gradient problem, because the skip connection provides a\n",
    "gradient-free highway that allows the network to transfer information forward\n",
    "uninterrupted.\n",
    "2. layer normalization is used in the Transformer block to provide stability to the\n",
    "training process. We have already seen the batch normalization layer in action throughout\n",
    "this book, where the output from each channel is normalized to have a mean of 0 and\n",
    "standard deviation of 1.\n",
    "3. Layer normalization. Each position within a sentence is normalized independently, but across the entire feature set for that position. This ensures that normalization happens independently for each word in a sentence, making it well-suited for sequential data like text. Use Case: Common in Transformers and other sequential models where the position-wise normalization provides stability and prevents dependencies on batch size.\n",
    "4. a set of feed-forward (i.e., densely connected) layers is included in the Transformer block, to allow the component to extract higher-level features as we go deeper into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ae6af92-ff21-4813-ab53-76f1034f47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    TransformerBlock is a custom Keras layer that implements a single block of the Transformer model.\n",
    "    \n",
    "    Attributes:\n",
    "        num_heads (int): Number of attention heads in the Multi-Head Attention layer.\n",
    "        key_dim (int): Dimensionality of the query and key vectors in attention.\n",
    "        embed_dim (int): Dimensionality of the input and output embeddings.\n",
    "        ff_dim (int): Dimensionality of the hidden layer in the feed-forward network (FFN).\n",
    "        dropout_rate (float): Dropout rate applied after the attention and FFN layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block with its components.\n",
    "\n",
    "        Args:\n",
    "            num_heads (int): Number of attention heads.\n",
    "            key_dim (int): Dimension of the query/key vectors.\n",
    "            embed_dim (int): Dimension of the output embeddings.\n",
    "            ff_dim (int): Dimension of the feed-forward layer.\n",
    "            dropout_rate (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Store input parameters as instance variables\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Define a Multi-Head Attention layer with the given number of heads and key dimension\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, output_shape=embed_dim)\n",
    "\n",
    "        # Dropout layer applied after the attention mechanism\n",
    "        self.dropout_1 = layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "        # Layer Normalization for the residual connection after attention\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # First layer of the Feed-Forward Network (FFN) with ReLU activation\n",
    "        self.ffn_1 = layers.Dense(units=ff_dim, activation='relu')\n",
    "\n",
    "        # Second layer of the Feed-Forward Network (FFN) to project back to the embedding dimension\n",
    "        self.ffn_2 = layers.Dense(units=embed_dim)\n",
    "\n",
    "        # Dropout layer applied after the FFN\n",
    "        self.dropout_2 = layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "        # Layer Normalization for the residual connection after the FFN\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "        \n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - Output tensor of shape (batch_size, seq_len, embed_dim).\n",
    "                - Attention scores tensor (for visualization or analysis).\n",
    "        \"\"\"\n",
    "        # Get the shape of the input tensor to use in creating a causal mask\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Create a causal attention mask to prevent the model from looking at future tokens\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "\n",
    "        # Apply Multi-Head Attention and mask to the inputs\n",
    "        attention_output, attention_scores = self.attn(inputs, inputs, attention_mask=causal_mask, return_attention_scores=True)\n",
    "\n",
    "        # Apply dropout to the attention output\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "\n",
    "        # Apply Layer Normalization and add a residual connection (skip connection)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "\n",
    "        # Pass the normalized output through the first FFN layer with ReLU activation\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "\n",
    "        # Pass the result through the second FFN layer to project it back to the embedding dimension\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "\n",
    "        # Apply dropout to the FFN output\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "\n",
    "        # Apply Layer Normalization and add a residual connection (skip connection)\n",
    "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the Transformer block for serialization.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Configuration dictionary containing the layer's parameters.\n",
    "        \"\"\"\n",
    "        # Get the base configuration from the parent class and update it with additional parameters\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                'num_heads': self.num_heads,\n",
    "                'key_dim': self.key_dim,\n",
    "                'embed_dim': self.embed_dim,\n",
    "                'ff_dim': self.ff_dim,\n",
    "                'dropout_rate': self.dropout_rate\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c6eb1-fa91-4b95-9771-16fc24063642",
   "metadata": {},
   "source": [
    "### Token and Position Embedding\n",
    "\n",
    "The token embedding is to convert each token into a learned vector. We create the positional embedding, using a\n",
    "standard Embedding layer to convert each integer position into a learned vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76ae85fa-52ae-48e7-970c-0080ff9e300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer that combines token embeddings and positional embeddings.\n",
    "\n",
    "    This layer maps input tokens and their positions into dense vectors of the same dimension,\n",
    "    which are then added together to produce the final embedding. This is typically used in \n",
    "    transformer-based models to provide both token and position information for sequential data.\n",
    "\n",
    "    Attributes:\n",
    "        max_len (int): Maximum sequence length of the input.\n",
    "        vocab_size (int): Size of the vocabulary for token embeddings.\n",
    "        embed_dim (int): Dimension of the embedding vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Initializes the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Args:\n",
    "            max_len (int): Maximum length of the input sequences.\n",
    "            vocab_size (int): Size of the vocabulary for the token embeddings.\n",
    "            embed_dim (int): Dimension of the output embedding vectors.\n",
    "        \"\"\"\n",
    "        # Call the parent constructor (Layer's __init__ method).\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "\n",
    "        # Store the maximum sequence length, vocabulary size, and embedding dimension.\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Create the token embedding layer:\n",
    "        # Maps each token (word) index to a dense embedding vector of size embed_dim.\n",
    "        # This is an embedding layer that transforms token indices (e.g., words) into dense vectors of size embed_dim.\n",
    "        # Each token from the vocabulary is mapped to a unique vector.\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "        # Create the position embedding layer:\n",
    "        # Maps each position index (0, 1, 2, ...) to a dense embedding vector of size embed_dim.\n",
    "        # This embedding layer assigns a unique embedding vector to each position index in the input sequence. \n",
    "        # Positional embeddings help the model learn the order of tokens, which is essential for sequential tasks.\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Combines token and positional embeddings for input sequences.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of token indices of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, sequence_length, embed_dim) with the \n",
    "            sum of token and positional embeddings.\n",
    "        \"\"\"\n",
    "        # Get the dynamic length of the input sequence (for padding or truncated sequences).\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "\n",
    "        # Create a tensor of position indices [0, 1, 2, ..., maxlen-1].\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        # Generate position embeddings for each position index in the sequence.\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        # Generate token embeddings for each token index in the input sequence.\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        # Add the token embeddings and positional embeddings element-wise.\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the layer for serialization.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the configuration of the layer.\n",
    "        \"\"\"\n",
    "        # Get the configuration of the parent class and update it with custom attributes.\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                'max_len': self.max_len,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'embed_dim': self.embed_dim\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c65976-7b8b-4d0e-9e6b-c9767e2899fa",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "474342f6-2469-4673-9b60-5061bdccb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the model configuration.\n",
    "MAX_LEN = 80               # Maximum length of the input sequence (time steps).\n",
    "VOCAB_SIZE = 10000         # Size of the vocabulary (number of unique tokens).\n",
    "EMBEDDING_DIM = 256        # Dimension of the embedding space (size of each token embedding vector).\n",
    "N_HEADS = 2                # Number of attention heads in the multi-head attention mechanism.\n",
    "KEY_DIM = 256              # Dimension of the query, key, and value vectors in the attention mechanism.\n",
    "FEED_FORWARD_DIM = 256     # Dimension of the hidden layer in the feed-forward network within the transformer block.\n",
    "\n",
    "# Define the input layer for the model.\n",
    "# Input shape: (batch_size, sequence_length), where sequence_length is flexible (None).\n",
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)  \n",
    "\n",
    "# Step 1: Apply token and position embeddings.\n",
    "# Inputs: token indices of shape (batch_size, sequence_length)\n",
    "# Outputs: Combined token and position embeddings of shape (batch_size, sequence_length, EMBEDDING_DIM)\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "\n",
    "# Step 2: Apply a transformer block to process the embeddings.\n",
    "# Inputs: Embedded tokens of shape (batch_size, sequence_length, EMBEDDING_DIM)\n",
    "# Outputs: \n",
    "#   - Processed embeddings of shape (batch_size, sequence_length, EMBEDDING_DIM)\n",
    "#   - Attention scores of shape (batch_size, N_HEADS, sequence_length, sequence_length)\n",
    "x, attention_scores = TransformerBlock(N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM)(x)\n",
    "\n",
    "# Step 3: Apply a Dense layer to project the output to the vocabulary size.\n",
    "# This is typically used in language models for predicting the next token in a sequence.\n",
    "# Inputs: Processed embeddings of shape (batch_size, sequence_length, EMBEDDING_DIM)\n",
    "# Outputs: Probability distribution over vocabulary of shape (batch_size, sequence_length, VOCAB_SIZE)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bebe6b0-1dc4-4da8-927d-c35fcb989fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the Adam optimizer and a sparse categorical cross-entropy loss function.\n",
    "# SparseCategoricalCrossentropy is used because the labels are integer indices, not one-hot vectors.\n",
    "gpt= models.Model(inputs=inputs, outputs=[outputs, attention_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf31fe-2888-43b8-bac8-e8a1803c7762",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "335785d8-3651-4c0d-a3a3-620b849a83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.compile(\n",
    "    'adam',\n",
    "    loss=[losses.SparseCategoricalCrossentropy(), None]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fff9a5f-20a9-4373-be76-70b3572a884e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,480</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)] │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m2,580,480\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    │       \u001b[38;5;34m658,688\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)] │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)    │     \u001b[38;5;34m2,570,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,809,168</span> (22.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,809,168\u001b[0m (22.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,809,168</span> (22.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,809,168\u001b[0m (22.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2040a2-c5ca-4a38-9c39-b9b424fa0076",
   "metadata": {},
   "source": [
    "Token and Position Embedding:\n",
    "- VOCAB_SIZE * EMBEDDING_DIM for token embeddings.\n",
    "- MAX_LEN * EMBEDDING_DIM for positional embeddings.\n",
    "- 2,580,480 = (10,000 * 256) + (80 * 256).\n",
    "\n",
    "Transformer Block:\n",
    "- Multi-head attention and feed-forward network parameters.\n",
    "\n",
    "Dense Layer:\n",
    "- (EMBEDDING_DIM * VOCAB_SIZE) = (256 * 10,000) = 2,570,000.\n",
    "\n",
    "Dimensional Flow:\n",
    "- Input: (batch_size, sequence_length)\n",
    "- After Embedding: (batch_size, sequence_length, 256)\n",
    "- After Transformer: (batch_size, sequence_length, 256)\n",
    "- After Dense Layer: (batch_size, sequence_length, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8671d5e-4be5-42c7-a4a1-bea3b6cd3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL= False\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    gpt= models.load_model('./models/gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950d9ac-eeb8-4bf7-8d29-e81bff02fbae",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4ab20-6f84-446d-80aa-84fa1ae059a4",
   "metadata": {},
   "source": [
    "### Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b4cc25a-cd62-494b-aadf-c1bb78854c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback for generating text during training.\n",
    "    \n",
    "    This callback generates text using the model's predictions after each training epoch.\n",
    "    \n",
    "    Args:\n",
    "        index_to_word (list): A list of words representing the vocabulary, where each index \n",
    "                              corresponds to a specific word.\n",
    "        top_k (int, optional): The number of top words to consider for sampling. Defaults to 10.\n",
    "    \n",
    "    Methods:\n",
    "        sample_from(probs, temperature):\n",
    "            Samples a word index from a probability distribution with temperature scaling.\n",
    "\n",
    "        generate(start_prompt, max_tokens, temperature):\n",
    "            Generates a sequence of text starting from a given prompt.\n",
    "\n",
    "        on_epoch_end(epoch, logs=None):\n",
    "            Called at the end of each epoch to generate text using the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        \"\"\"\n",
    "        Initializes the TextGenerator callback.\n",
    "        \n",
    "        Args:\n",
    "            index_to_word (list): Vocabulary mapping from index to word.\n",
    "            top_k (int): Number of top predictions to consider for sampling. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.index_to_word = index_to_word  # Vocabulary list mapping indices to words.\n",
    "        # Create a reverse mapping from word to index for fast lookup.\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        \"\"\"\n",
    "        Samples an index from a probability distribution after scaling it with temperature.\n",
    "        \n",
    "        Args:\n",
    "            probs (np.ndarray): An array of probabilities for each vocabulary word.\n",
    "            temperature (float): The temperature value for controlling randomness in sampling.\n",
    "                                 A higher temperature increases diversity of generated text,\n",
    "                                 while lower values make it more deterministic.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A sampled index and the modified probability distribution.\n",
    "        \"\"\"\n",
    "        # Adjust the probability distribution using temperature scaling.\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)  # Normalize the probabilities.\n",
    "\n",
    "        # Sample an index from the adjusted probability distribution.\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        \"\"\"\n",
    "        Generates text starting from a prompt and continues until reaching max_tokens or an end token.\n",
    "        \n",
    "        Args:\n",
    "            start_prompt (str): The initial text prompt to start generating from.\n",
    "            max_tokens (int): The maximum number of tokens to generate.\n",
    "            temperature (float): Temperature value for controlling randomness during sampling.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing information about each generated token,\n",
    "                  including its probability and attention scores.\n",
    "        \"\"\"\n",
    "        # Convert the start prompt into a list of token indices.\n",
    "        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
    "\n",
    "        sample_token = None  # Placeholder for the next sampled token.\n",
    "        info = []  # List to store information about each generated token.\n",
    "\n",
    "        # Continue generating tokens until reaching max_tokens or sampling an end token (index 0).\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            # Convert the list of tokens into a NumPy array with shape (1, sequence_length).\n",
    "            x = np.array([start_tokens])\n",
    "\n",
    "            # Use the model to predict the next token and its attention scores.\n",
    "            y, att = self.model.predict(x, verbose=0)\n",
    "\n",
    "            # Sample the next token from the predicted probabilities.\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
    "\n",
    "            # Append information about the current generation step.\n",
    "            info.append(\n",
    "                {\n",
    "                    'prompt': start_prompt,        # Original prompt used.\n",
    "                    'word_probs': probs,           # Probability distribution for the next word.\n",
    "                    'atts': att[0, :, -1, :]       # Attention scores for the generated token.\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Add the sampled token to the list of tokens.\n",
    "            start_tokens.append(sample_token)\n",
    "\n",
    "            # Update the start prompt by appending the new word.\n",
    "            start_prompt = start_prompt + ' ' + self.index_to_word[sample_token]\n",
    "\n",
    "        # Print the final generated text after the loop completes.\n",
    "        print(f'\\ngenerated text:\\n{start_prompt}\\n')\n",
    "\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Called automatically by Keras at the end of each training epoch.\n",
    "        Generates and prints a sample text starting from a predefined prompt.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): The current epoch number.\n",
    "            logs (dict, optional): A dictionary containing training metrics and information.\n",
    "        \"\"\"\n",
    "        self.generate('wine review', max_tokens=80, temperature=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46a633-3b5d-4aca-83ae-c89d0415f301",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d93002-bb04-46ec-8666-8d9427ceb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback= callbacks.ModelCheckpoint(\n",
    "    filepath='./checkpoints/gpt-checkpoint.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30b9d2dc-ca86-40fa-b456-953fe30f2b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 07:30:11.046252: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-11-28 07:30:11.046294: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-11-28 07:30:11.048249: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1239] Profiler found 1 GPUs\n",
      "2024-11-28 07:30:11.053691: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:137] cuptiGetTimestamp: error 999: \n",
      "2024-11-28 07:30:11.053730: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.\n",
      "2024-11-28 07:30:11.053734: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-28 07:30:11.053738: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1281] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "2024-11-28 07:30:11.053901: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-11-28 07:30:11.054164: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.\n",
      "2024-11-28 07:30:11.054173: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-28 07:30:11.054176: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1373] function cupti_interface_->Finalize()failed with error \n"
     ]
    }
   ],
   "source": [
    "log_dir='./logs/fit/gpt/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "tensorboard_callback= callbacks.TensorBoard(\n",
    "    log_dir= log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd32b5f-7ad4-4059-8e12-25199b6c2942",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7788d5a1-57be-493c-85f5-53a6fe07bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize starting prompt\n",
    "text_generator= TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb1519-0e61-4891-b43a-35088600c6c4",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f5507be-f2bf-4e44-8b48-a88c76ee7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 07:30:13.226111: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-28 07:30:13.492551: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   6/4060\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 38ms/step - loss: 8.6526"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 07:30:22.521067: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-11-28 07:30:22.521098: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-11-28 07:30:22.521115: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.521120: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.521122: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.521125: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1281] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "2024-11-28 07:30:22.553701: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-11-28 07:30:22.562344: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.562397: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.562402: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1373] function cupti_interface_->Finalize()failed with error \n",
      "2024-11-28 07:30:22.565303: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.565350: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-11-28 07:30:22.565376: I external/local_xla/xla/backends/profiler/gpu/cupti_collector.cc:540]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2024-11-28 07:30:22.566967: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.5890\n",
      "generated text:\n",
      "wine review : a germany : pfalz , saar impression as it ' s a weighty yet thankfully so soft and chocolaty wine , with the acidity and flavors of dusty fruit and filled out . the citrus note signals the label or so , which it ' s a fine example of a properly delicate level of acidity , balance and keep it fine now for improvement . \n",
      "\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 86ms/step - loss: 2.5889\n",
      "Epoch 2/5\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.9773\n",
      "generated text:\n",
      "wine review : black - currant : western cape : sauvignon blanc de [UNK] boasts lovers of moscato d ' oro , its stylish elements only be found in this bottling from a popular \" \" \" \" \" dry \" \" \" - style \" first , \" \" of one \" \" traditions , and louis sbs , david lett ) but . it ' s a fine wine , with a whiff of sulfur at the dinner\n",
      "\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 136ms/step - loss: 1.9773\n",
      "Epoch 3/5\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.8944\n",
      "generated text:\n",
      "wine review : the olsen : razor crest blend : bordeaux blends as the focus in the [UNK] of reininger ' s 2010 vintages to [UNK] and released by this bottle barolo is fully evident in the pale peach aromas and mineral scents . let it breathe and unfolds well , adding a black cherry - currant flavor and a tangy citrus linger long on the finish . \n",
      "\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 127ms/step - loss: 1.8944\n",
      "Epoch 4/5\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.8517\n",
      "generated text:\n",
      "wine review : a level of pink floral rosé : rich with hints of green pepper , this dry rosé has a hint of sweetness . on the mid - palate will pair with next few months a lively orange note . \n",
      "\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 126ms/step - loss: 1.8517\n",
      "Epoch 5/5\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.8241\n",
      "generated text:\n",
      "wine review : there are germany : here ' s : a surprisingly subtle wine that a [UNK] . an delicately tannic yet dry wine that bristling and fruity development ahead of . it ' s modest raspberry and cherry notes are just beginning to mellow out . drink now . \n",
      "\n",
      "\u001b[1m4060/4060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 126ms/step - loss: 1.8241\n",
      "CPU times: user 45min 54s, sys: 35.9 s, total: 46min 30s\n",
      "Wall time: 40min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fe038c2ddd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 5 epochs ~ 46 min 4060/4060 127ms/step\n",
    "gpt.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        text_generator\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c57a6d9-63e1-423c-aa71-d26a5578ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.save('./models/gpt.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3f937-e671-4e0f-92f9-bbb1eea96b10",
   "metadata": {},
   "source": [
    "## GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3fe12ae-d21f-4e39-8574-99903242aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "wine review: italy di italy : piedmont : nebbiolo : this opens with aromas of ripe dark berry , toast , espresso and a whiff of oak . the firm palate delivers ripe black cherry , licorice , tobacco and clove alongside assertive tannins . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info= text_generator.generate('wine review: italy', max_tokens=80, temperature=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f83c27c-5cd8-45fb-98ea-ac7043649d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "wine review: germany . new zealand : marlborough : pinot grigio : this simple , medium - bodied pinot grigio it is silky and easy , but the citrus - fruit flavors are complemented by a silky texture . drink now . \n",
      "\n",
      "CPU times: user 1.64 s, sys: 169 ms, total: 1.8 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info= text_generator.generate('wine review: germany', max_tokens=80, temperature=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f883e3-b7b8-4121-8e10-2734730ca6ff",
   "metadata": {},
   "source": [
    "## REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f892aee-459a-4e9b-af93-7ef8a0405f52",
   "metadata": {},
   "source": [
    "1. [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/): David Foster's book from which has become an inspiration of this notebook.\n",
    "2. [David Foster](https://github.com/davidADSP): GitHub page\n",
    "3. [David Foster (Keynote) - Generative Deep Learning -Key To Unlocking Artificial General Intelligence?](https://www.youtube.com/watch?v=rHLf78CmNmQ): David's video session at Youtube regarding some key concepts has written in his book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4eb1eb-c565-477a-b890-1c25dbd65dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
