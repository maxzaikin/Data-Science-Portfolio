{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d9e4fa-4537-406c-89f7-dbf7e03ca7c2",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600ea52-768a-4bac-a788-53b311cf55f5",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc942bb-64bc-4acf-870b-65cebad04a4c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "514987ff-ab57-41bd-9429-7efd0e048530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db07681-d157-4b6a-a4e6-b615842891ae",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a050a5ca-5daf-41b1-941d-85fdb3a818f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(s):\n",
    "    \"\"\"\n",
    "    Adds spaces around punctuation symbols in a string and normalizes whitespace.\n",
    "\n",
    "    This function takes an input string, identifies all punctuation characters,\n",
    "    and surrounds them with spaces. It then removes any instances of multiple\n",
    "    consecutive spaces, ensuring the string is neatly formatted.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    s : str\n",
    "        The input string to process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The processed string with spaces around punctuation and normalized whitespace.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> pad_punctuation(\"Hello,world! This is a test.\")\n",
    "    'Hello , world ! This is a test .'\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function uses Python's `string.punctuation` to identify all standard punctuation symbols.\n",
    "    - Regular expressions are used to handle the replacement efficiently.\n",
    "    \"\"\"\n",
    "    \n",
    "    # buld a puctuation symbol regular expression, and then replace coughted puctuation symbol with ' symbol '\n",
    "    s= re.sub(f'([{string.punctuation}])', r' \\1 ', s)\n",
    "    # replace all occurance of more the 1 whitespace in the row\n",
    "    return re.sub(' +', ' ', s)   \n",
    "\n",
    "def prepare_inputs(text):\n",
    "    \"\"\"\n",
    "    Prepares input and target tensors for a token prediction model.\n",
    "\n",
    "    This transformation processes a batch of text data to help the model learn to predict \n",
    "    the next token in a sentence by analyzing previous tokens within the same sentence.\n",
    "\n",
    "    Args:\n",
    "        text (tf.Tensor): A batch of raw text input, typically as a 1D tensor or list of strings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (x, y) where:\n",
    "            - x (tf.Tensor): The input tensor containing all tokens of each sentence \n",
    "              except the last token, with shape (batch_size, sequence_length-1).\n",
    "            - y (tf.Tensor): The target tensor containing all tokens of each sentence \n",
    "              starting from the second token, with shape (batch_size, sequence_length-1).\n",
    "    \n",
    "    Steps:\n",
    "        1. Expands the text tensor by adding a new dimension at the end.\n",
    "        2. Tokenizes the sentences using the provided `vectorize_layer`.\n",
    "        3. Splits the tokenized sentences into:\n",
    "            - `x`: All tokens except the last one.\n",
    "            - `y`: All tokens except the first one.\n",
    "    \"\"\"\n",
    "    # Add 1 dimension to the text data for compatibility with the vectorizer\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    \n",
    "    # Tokenize sentences using the vectorize_layer (assumed to be predefined)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "\n",
    "    # Input tensor: all tokens except the last one\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    \n",
    "    # Target tensor: all tokens starting from the second one\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Generates a causal attention mask for Transformer-based models.\n",
    "\n",
    "    A causal mask ensures that for each destination (decoder) token, the model can only\n",
    "    attend to source (encoder) tokens that have already been processed or align in time.\n",
    "    This is critical for tasks such as autoregressive generation where future tokens should\n",
    "    not be visible.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences in the batch.\n",
    "        n_dest (int): Number of destination tokens (e.g., in the decoder).\n",
    "        n_src (int): Number of source tokens (e.g., in the encoder or the decoder itself).\n",
    "        dtype (tf.DType): Data type for the resulting mask (e.g., `tf.float32`).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A mask tensor of shape `(batch_size, n_dest, n_src)` with values of 1.0\n",
    "                   for allowed positions and 0.0 for disallowed positions.\n",
    "    \"\"\"\n",
    "    # Create a range of indices for destination tokens and reshape to a column vector\n",
    "    i = tf.range(n_dest)[:, None]  # Shape: (n_dest, 1)\n",
    "    \n",
    "    # Create a range of indices for source tokens\n",
    "    j = tf.range(n_src)  # Shape: (n_src,)\n",
    "    \n",
    "    # Compare indices to establish causality: i >= j - offset\n",
    "    # This ensures the mask allows only past and current positions\n",
    "    m = i >= j - n_src + n_dest  # Shape: (n_dest, n_src)\n",
    "    \n",
    "    # Cast the boolean mask into the specified dtype (e.g., float32 or float16)\n",
    "    mask = tf.cast(m, dtype)  # Shape: (n_dest, n_src)\n",
    "    \n",
    "    # Reshape the mask to add a singleton batch dimension\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])  # Shape: (1, n_dest, n_src)\n",
    "    \n",
    "    # Calculate the multiplier for tiling the mask to match batch size\n",
    "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
    "    # `mult` is a vector specifying how many times to replicate along each axis\n",
    "    \n",
    "    # Tile the mask along the batch dimension\n",
    "    return tf.tile(mask, mult)  # Final shape: (batch_size, n_dest, n_src)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762c44c-40ab-455a-b2aa-1165f491ab2d",
   "metadata": {},
   "source": [
    "## PREPARE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305593d-e66e-4d26-8693-6bad88cd2eb4",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdde2842-336a-424a-923d-97fe65352d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6e370-48a3-4115-a409-65284cb782b7",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f173f7-3700-4fb5-a516-4d1f5c003ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/wine-reviews/winemag-data-130k-v2.json') as json_data:\n",
    "    wine_data= json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7352e7-6643-4eed-b068-a0a0d011f957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff43c97-7d50-4d52-9c40-31038d0a7c4e",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84327d16-c973-4729-9a80-f63f29d8212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets\n",
    "filtered_data= [\n",
    "    'wine_review : ' + x['country'] + ' : ' + x['province'] + ' : ' + x['variety'] + ' : ' + x['description'] for x in wine_data \n",
    "    if x['country'] is not None\n",
    "       and x['province'] is not None\n",
    "       and x['variety'] is not None\n",
    "       and x['description'] is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef73e9f-42a7-40d9-893b-01e2274db65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine_review : US : California : Cabernet Sauvignon : Soft, supple plum envelopes an oaky structure in this Cabernet, supported by 15% Merlot. Coffee and chocolate complete the picture, finishing strong at the end, resulting in a value-priced wine of attractive flavor and immediate accessibility.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadca3d9-71c6-4a12-ac20-ac3cf453d907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129907 recepies loaded\n"
     ]
    }
   ],
   "source": [
    "n_wines= len(filtered_data)\n",
    "print(f'{n_wines} recepies loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fca18-76c3-4d30-a74f-4d0751b57422",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43c305-017c-41b7-a5fc-6e302557d10d",
   "metadata": {},
   "source": [
    "### Pad the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9e57e1f-f804-4e61-8353-2be5683d0208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.56 s, sys: 10.2 ms, total: 3.57 s\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~3s\n",
    "text_data= [pad_punctuation(x) for x in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e411dad5-b63e-4ff0-8a08-1e3bc0e0a679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine _ review : US : California : Pinot Noir : Oak and earth intermingle around robust aromas of wet forest floor in this vineyard - designated Pinot that hails from a high - elevation site . Small in production , it offers intense , full - bodied raspberry and blackberry steeped in smoky spice and smooth texture . '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data= text_data[25]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3560ef-4889-4dee-967e-80da846aaabd",
   "metadata": {},
   "source": [
    "### Convert to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81edd78b-aa76-4f04-99a9-00cd3482a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 22:09:54.060761: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.265071: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.265131: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.269764: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.269813: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.269834: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.542520: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.542879: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.542896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-26 22:09:54.543017: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 22:09:54.543148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2245 MB memory:  -> device: 0, name: NVIDIA T1200 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Convert data to a TensorFlow dataset devided by batches with 32 recepies and shuffle buffer thus all recepies are devided randomly\n",
    "text_ds= (\n",
    "    tf.data.Dataset.from_tensor_slices(text_data).batch(32).shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b386723-e206-40f0-8f2c-dd23ac75c8f5",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c2457-1b36-4175-a764-870bba96a574",
   "metadata": {},
   "source": [
    "#### Create Vect. layer\n",
    "\n",
    "Create a Keras TextVectorization layer:\n",
    "- convert text to lowercase\n",
    "- give most prevalent 10k words a corresponding integer token\n",
    "- pad the sequnce to 81 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc9ef583-29ec-4b3c-9de2-e868fd67cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=10000\n",
    "MAX_LEN=80\n",
    "\n",
    "vectorize_layer= layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=80 + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2a306-67fd-4776-a349-7b48dd677c6f",
   "metadata": {},
   "source": [
    "#### Calc text statistics\n",
    "\n",
    "- Apply TextVectorization to the training data\n",
    "- Get the vocabulary of 10k most pevalent words.  \n",
    "  NOTE:  \n",
    "  - all words over 10k will be coded as 1 (i.e. UNK)\n",
    "  - if number of words in sentence less then 201, thouse will be coded as 0 (i.e. stop token - text string come to an end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35c94748-ecbc-4026-b160-07c082eee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 3.64 s, total: 20.1 s\n",
      "Wall time: 19.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 22:34:27.669896: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~20s \n",
    "# Adapt layer to the training set\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab=vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96806a46-2486-4460-9fd5-f6e9a7bbf7a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : \n",
      "1 : [UNK]\n",
      "2 : :\n",
      "3 : ,\n",
      "4 : .\n",
      "5 : and\n",
      "6 : the\n",
      "7 : wine\n",
      "8 : a\n",
      "9 : of\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(vocab[:10]):\n",
    "    print(f'{i} : {word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432c9d4-32f7-4ec3-80c8-274781f38cba",
   "metadata": {},
   "source": [
    "INTERIM CONCLUSION\n",
    "\n",
    "We see a subset of tokens mapped to their respctive indices. The layer reserves the 0 token for padding, and 1 for unknown. NOTE:\n",
    "\n",
    "    The other words are assigned tokens in order of frequency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9f857e1-756c-4c80-a97e-4894d45dc337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine _ review : US : Oregon : Pinot Gris : Tart and snappy , the flavors of lime flesh and rind dominate . Some green pineapple pokes through , with crisp acidity underscoring the flavors . The wine was all stainless - steel fermented . '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e888286-bb70-4a8a-a7ee-9e59a0442bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   7   11   10    2   21    2  151    2   44  411    2  139    5 1009\n",
      "    3    6   17    9  150 1030    5  681  627    4  105   95  235 6405\n",
      "   85    3   12   74   31 5782    6   17    4    6    7  440  128  879\n",
      "   15  797  542    4    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# display same as above but as converted to int word mappings\n",
    "example_tokenised=vectorize_layer(text_data[2])\n",
    "print(example_tokenised.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd077b8-11a6-4ad4-82b5-75a8cffff654",
   "metadata": {},
   "source": [
    "#### Create Training Dataset\n",
    "\n",
    "prepare_inputs convert the dataset to the MapDataset where each sentnce is splited on to the 2 sets of sequences:\n",
    "- x: contains all words in sentece except last\n",
    "- y: shifted by one left, thus it starts from the 2nd element\n",
    "\n",
    "Thus we will have a tuple `[x,y]` thus when our model will train it will learn relation ships between words as it nos that word `x` the target will be `y`. For example in sentence `The cloud is white` model will learn that `x=The` and `y=cloud` so it will adjust it weight accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56597fbd-87c1-4f00-b09a-d65619db3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds= text_ds.map(prepare_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cc5b18a-e79e-4bcc-9d36-f36694d4760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([   7,   11,   10,    2,   41,    2,  333,   41,    2,  164,    2,\n",
       "         13,  187,  466,    9,  164,   76,   53,  796,  552,    9,   27,\n",
       "         26, 8090,   33,   73, 2925,   50,   20,  464,  289,   20,   95,\n",
       "        131,    4,    8,  109,    9,  214,  122,  908,  266,    6,    7,\n",
       "          8,   88,    3, 4671,  615,    4,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_output= train_ds.take(1).get_single_element()\n",
    "example_input_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65cc671c-2b50-4e7e-9e6a-643e7582148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([   7,   11,   10,    2,   86,    2,  209,   86,    2,  272,    2,\n",
       "       1009,   37,    5,   60,   27,  328,    6, 1955,  683,    9,  341,\n",
       "          5,  741,    4,   16,  211,   57,    3,  343,    5,  181,    3,\n",
       "         12,   68,    5,  124,   60,   17,    4,    6,   32,  788,   97,\n",
       "          5,  103,    4,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_output[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ed590-ca80-47cb-a4e6-c765b37c6e83",
   "metadata": {},
   "source": [
    "#### Creat the causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ab1a0db-22cd-4efe-b123-2333d523b7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(causal_attentin_mask(1,10,10, dtype=tf.int32)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b370e1f-22fc-41dc-952c-073223f930f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
