{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef1976e-597a-4f85-9a73-b955948faf60",
   "metadata": {},
   "source": [
    "# LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f19398-c202-4f4d-adc7-7e8a3f8fab75",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e4a1c-8689-4103-9e62-f1caff191e07",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0c5f7e-92c4-4b07-a214-8285e16a0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 23:19:16.394422: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-27 23:19:16.464620: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-27 23:19:16.464654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-27 23:19:16.473434: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-27 23:19:16.495026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    losses,\n",
    "    callbacks\n",
    ")\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b8660f-2bb7-450c-bbf4-8091aadde1f2",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdbd311b-9a27-42b2-9002-9ddf7d8f05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(s):\n",
    "    # build a punctuation symbol regular expression, and then replace coughted punctuation symbol (kept in \\1) with that symbol surounded by single whitespaces\n",
    "    s=re.sub(f'([{string.punctuation}])', r' \\1 ', s)\n",
    "    # replace all occurance of more than 1 whitespaces in a row with one whitespace`\n",
    "    s=re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def prepare_inputs(text):\n",
    "    '''\n",
    "    This transformation helps model learn to predict next token in sentence by analyzing previous tokens in same sentence\n",
    "    Returns:\n",
    "        x,y: Pair where y is the target token for x\n",
    "    '''\n",
    "    # Add 1 dimension in the end of text data\n",
    "    text=tf.expand_dims(text,-1) \n",
    "    tokenized_sentences=vectorize_layer(text)\n",
    "\n",
    "    # Create input tensor=x which contains all tokens of each sentence except last token in each sentence\n",
    "    x=tokenized_sentences[:,:-1]\n",
    "    # Create target tensor=y which contains all tokens but starting from the 2nd\n",
    "    y=tokenized_sentences[:,1:]\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "def print_probs(info, vocab, top_k=5):\n",
    "    '''\n",
    "    Print probabilitis for the words with highest probablities based on model's experience, which has been learnt during inference.\n",
    "\n",
    "    Args:\n",
    "        info (list): Data gerated from TextGenerator.generate function\n",
    "                     e.g.   [{'prompt'    : 'recipe for chockolate ice cream |',\n",
    "                              'word_probs': array([9.1210980e-13, 3.9852460e-14, 1.2957362e-18, ..., 0.0000000e+00,\n",
    "                                     2.9555584e-37, 1.7050197e-32], dtype=float32)}]\n",
    "        vocab (list): Model vocabulary\n",
    "        top_k (int): Number of words with highest probability\n",
    "    Returns:\n",
    "        PROMPT   : text | generated text\n",
    "                precicted_word_1:   \tpredicted_probability_%\n",
    "                precicted_word_2:   \tpredicted_probability_%\n",
    "                precicted_word_3:   \tpredicted_probability_%\n",
    "                precicted_word_4:   \tpredicted_probability_%\n",
    "                precicted_word_5:   \tpredicted_probability_%\n",
    "                ----------\n",
    "    '''\n",
    "    for i in info:\n",
    "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
    "        word_probs=i['word_probs']\n",
    "        # Sort, reverse and take first top_k elements\n",
    "        p_sorted=np.sort(word_probs)[::-1][:top_k]\n",
    "        # Sort, reverse and take first top_k elements\n",
    "        i_sorted=np.argsort(word_probs)[::-1][:top_k]\n",
    "\n",
    "        for p,i in zip(p_sorted,i_sorted):\n",
    "            # Print probabilities for each element in percents\n",
    "            print(f'{vocab[i]}:   \\t{np.round(100*p,2)}%')\n",
    "\n",
    "        print('----------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be5225-7c56-4099-a4ba-7e8b56c18259",
   "metadata": {},
   "source": [
    "## PREPARE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db26a7-ddf3-4359-a8ca-4a69278f9ece",
   "metadata": {},
   "source": [
    "### Load and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7516b960-4be1-43b8-85c6-71788ca9ffe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./datasets/epirecipes/full_format_recipes.json') as json_data:          \n",
    "   recipe_data=json .load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f6861e-2cb8-49d1-896f-2e93b0fd44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of objects in the list: 20111\n"
     ]
    }
   ],
   "source": [
    "# create filtered data using list comprehensions\n",
    "filtered_data=[\n",
    "    'Recipe for ' + x['title'] + ' | ' + \" \".join(x['directions'])\n",
    "    for x in recipe_data\n",
    "    if 'title' in x\n",
    "    and x['title'] is not None\n",
    "    and 'directions' in x\n",
    "    and x['directions'] is not None\n",
    "]\n",
    "\n",
    "print(f'Num of objects in the list: {len(filtered_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1495503c-0a27-40a2-b224-9315c2e9357f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recipe for Boudin Blanc Terrine with Red Onion Confit  | Combine first 9 ingredients in heavy medium saucepan. Add 3 shallots. Bring to simmer. Remove from heat, cover and let stand 30 minutes. Chill overnight. Preheat oven to 325Â°F. Line 7-cup pÃ¢tÃ© or bread pan with plastic wrap. Melt butter in heavy small skillet over low heat. Add remaining 5 shallots. Cover and cook until very soft, stirring occasionally, about 15 minutes. Transfer to processor. Add pork, eggs, flour and Port and puree. Strain cream mixture, pressing on solids to extract as much liquid as possible. With processor running, add cream through feed tube and process just until combined with pork. Transfer to large bowl. Mix in currants. Spoon mixture into prepared pan. Cover with foil. Place pan in large pan. Add boiling water to larger pan to within 1/2 inch of top of terrine. Bake until terrine begins to shrink from sides of pan and knife inserted into center comes out clean, about 1 1/2 hours. Uncover and cool on rack. Chill until cold. (Can be made 3 days ahead. Cover and chill.) Line platter with lettuce. Arrange terrine atop. Sprinkle with pepper and parsley. Garnish with bay leaves. Spoon confit around sides. Serve with bread. Heat oil in heavy large skillet over medium-high heat. Add onions and sautÃ© until crisp-tender, about 8 minutes. Add all remaining ingredients and stir until reduced to thick glaze, about 4 minutes. Season with salt and pepper. (Can be prepared 2 days ahead. Cover and chill.) Serve warm or at room temperature.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258d76e-a2f5-4fd5-ad9c-0263a122281c",
   "metadata": {},
   "source": [
    "**INTERIM CONCLUSION**\n",
    "\n",
    "We have succesfully loaded json formated data, filtered it and pre-process, thus after all transormation being done, we haeve a filtered list named `filtered data`, which contains 20111 filtered objects with title begins from `recipe for` and directions separated from title by the `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f32ce1-fc24-4639-a6dd-7280f0b5ac35",
   "metadata": {},
   "source": [
    "## TOKENIZATION\n",
    "\n",
    "Tokenization is the process of a breaking the text up into individual units, such as words or little pieces of words or charachter symbols or other common character combinations.\n",
    "\n",
    "So how to tokenize your text will depend on final goals, but always keep in mind that key strategy here is how you want your model process the text. For example you can lowercase all words, and thus all words Bill and bill will be pointed to the same index, this is good when your text is desired to someone whose named Bill, but in case if this text has something related to Bills finanace operations and bill your model will loos this conection, which in some cases may be important, so `know your corpus`  \n",
    "Main Questions to answer:\n",
    "- Capitalize or Not\n",
    "- Size of vocabulary\n",
    "- Stemm or Not\n",
    "- Tokenize punctuation or Not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67bb73-622b-4058-912b-d66f96801bc2",
   "metadata": {},
   "source": [
    "### Pad the punctuation\n",
    "\n",
    "Pad the punctuation to treat them as separarate words\n",
    "\n",
    "```python\n",
    "def pad_punctuation(s):\n",
    "    # build a punctuation symbol regular expression, and then replace coughted punctuation symbol (kept in \\1) with that symbol surounded by single whitespaces\n",
    "    s=re.sub(f'([{string.punctuation}])', r' \\1 ', s)\n",
    "    # replace all occurance of more than 1 whitespaces in a row with one whitespace`\n",
    "    s=re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dc27ef0-e89d-4cb7-a1c2-2dbc71c4c59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recipe for Boudin Blanc Terrine with Red Onion Confit  | Combine first 9 ingredients in heavy medium saucepan. Add 3 shallots. Bring to simmer. Remove from heat, cover and let stand 30 minutes. Chill overnight. Preheat oven to 325Â°F. Line 7-cup pÃ¢tÃ© or bread pan with plastic wrap. Melt butter in heavy small skillet over low heat. Add remaining 5 shallots. Cover and cook until very soft, stirring occasionally, about 15 minutes. Transfer to processor. Add pork, eggs, flour and Port and puree. Strain cream mixture, pressing on solids to extract as much liquid as possible. With processor running, add cream through feed tube and process just until combined with pork. Transfer to large bowl. Mix in currants. Spoon mixture into prepared pan. Cover with foil. Place pan in large pan. Add boiling water to larger pan to within 1/2 inch of top of terrine. Bake until terrine begins to shrink from sides of pan and knife inserted into center comes out clean, about 1 1/2 hours. Uncover and cool on rack. Chill until cold. (Can be made 3 days ahead. Cover and chill.) Line platter with lettuce. Arrange terrine atop. Sprinkle with pepper and parsley. Garnish with bay leaves. Spoon confit around sides. Serve with bread. Heat oil in heavy large skillet over medium-high heat. Add onions and sautÃ© until crisp-tender, about 8 minutes. Add all remaining ingredients and stir until reduced to thick glaze, about 4 minutes. Season with salt and pepper. (Can be prepared 2 days ahead. Cover and chill.) Serve warm or at room temperature.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d1dcec-99e7-4d1e-95ed-6f5a50e4c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data=[pad_punctuation(x) for x in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce082d5b-740b-4cd9-988d-d545104e4a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recipe for Boudin Blanc Terrine with Red Onion Confit | Combine first 9 ingredients in heavy medium saucepan . Add 3 shallots . Bring to simmer . Remove from heat , cover and let stand 30 minutes . Chill overnight . Preheat oven to 325Â°F . Line 7 - cup pÃ¢tÃ© or bread pan with plastic wrap . Melt butter in heavy small skillet over low heat . Add remaining 5 shallots . Cover and cook until very soft , stirring occasionally , about 15 minutes . Transfer to processor . Add pork , eggs , flour and Port and puree . Strain cream mixture , pressing on solids to extract as much liquid as possible . With processor running , add cream through feed tube and process just until combined with pork . Transfer to large bowl . Mix in currants . Spoon mixture into prepared pan . Cover with foil . Place pan in large pan . Add boiling water to larger pan to within 1 / 2 inch of top of terrine . Bake until terrine begins to shrink from sides of pan and knife inserted into center comes out clean , about 1 1 / 2 hours . Uncover and cool on rack . Chill until cold . ( Can be made 3 days ahead . Cover and chill . ) Line platter with lettuce . Arrange terrine atop . Sprinkle with pepper and parsley . Garnish with bay leaves . Spoon confit around sides . Serve with bread . Heat oil in heavy large skillet over medium - high heat . Add onions and sautÃ© until crisp - tender , about 8 minutes . Add all remaining ingredients and stir until reduced to thick glaze , about 4 minutes . Season with salt and pepper . ( Can be prepared 2 days ahead . Cover and chill . ) Serve warm or at room temperature . '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f452c-604c-4f47-a4eb-ec0aa02343d6",
   "metadata": {},
   "source": [
    "**INTERIM CONCLUSION**\n",
    "\n",
    "We have successfully padded punctuation symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009d879-fe18-4e6d-a638-f6925cccb1ec",
   "metadata": {},
   "source": [
    "### Convert to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf1c7288-ba93-4fba-947a-704818c15ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to a TensorFlow dataset devided by batches with 32 recepies and shuffle buffer thus all recepies are devided randomly\n",
    "text_ds=(\n",
    "    tf.data.Dataset.from_tensor_slices(text_data)\n",
    "    .batch(32)\n",
    "    .shuffle(1000)\n",
    ")\n",
    "\n",
    "#for batch in text_ds.take(1):\n",
    "#    print(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc4d6e-d30a-4821-876c-35521a2fcde8",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa33a1-cfa7-40b7-867e-c33577e1b3f4",
   "metadata": {},
   "source": [
    "#### Create vectorization layer\n",
    "\n",
    "Create a Keras TextVectorization layer:\n",
    "- to convert text to lowercase\n",
    "- give most prevalent 10k words a correcponding integer token\n",
    "- pad the sequnce to 201 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a01e0e-0a97-436e-9eaf-bbfd872d9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=10000\n",
    "MAX_LEN=200\n",
    "\n",
    "vectorize_layer=layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba8199-8bee-4bfb-828f-6275d4f4106b",
   "metadata": {},
   "source": [
    "#### Calculate text statistics\n",
    "\n",
    "i.e.  \n",
    "- Apply TextVectorization to the training data\n",
    "- Get the vocabulary of 10k most pevalent words.  \n",
    "  NOTE:  \n",
    "  - all words over 10k will be coded as 1 (i.e. UNK)\n",
    "  - if number of words in sentence less then 201, thouse will be coded as 0 (i.e. stop token - text string come to an end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc8c522-6bc0-484f-b928-d06d5301446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.41 s, sys: 802 ms, total: 5.21 s\n",
      "Wall time: 5.38 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:47:25.998507: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab=vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d7680b-54bb-4b2d-9fcc-6d0dcfc1a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: .\n",
      "3: ,\n",
      "4: and\n",
      "5: to\n",
      "6: in\n",
      "7: the\n",
      "8: with\n",
      "9: a\n"
     ]
    }
   ],
   "source": [
    "# Display token-word mapping\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f'{i}: {word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23fa81-8f16-4033-865d-378f393f1f18",
   "metadata": {},
   "source": [
    "**INTERIM CONCLUSION**\n",
    "\n",
    "We see a subset of tokens mapped ti their respctive indices. The layer reserves the 0 token for padding, and 1 for unknown.\n",
    "NOTE:\n",
    "- The other words are assigned tokens in order of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cc6d3a0-3f46-4ae9-87a8-8988d833c310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recipe for Potato and Fennel Soup Hodge | In a large heavy saucepan cook diced fennel and onion in butter over moderate heat , stirring , until softened , about 10 minutes . Peel and cube potatoes . Add potatoes and broth to fennel mixture and simmer , covered , until potatoes are very tender , about 20 minutes . In a blender or food processor purÃ©e mixture in batches until smooth and return to saucepan . Stir in milk and salt and pepper to taste and simmer soup , stirring occasionally , 10 minutes , or until heated through . Garnish soup with reserved fennel leaves . '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ecfe773-ca1f-45cb-b687-1d7f34b281d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  26   16  335    4  354  244    1   27    6    9   30   78   80   43\n",
      " 1282  354    4  115    6   50   20  269   17    3   48    3   10  361\n",
      "    3   19   82   12    2  175    4 1915  150    2   18  150    4  171\n",
      "    5  354   31    4   70    3  121    3   10  150   79  218   85    3\n",
      "   19  170   12    2    6    9  281   41  291  188  324   31    6  303\n",
      "   10  141    4  246    5   80    2   42    6  211    4   24    4   33\n",
      "    5  132    4   70  244    3   48   90    3   82   12    3   41   10\n",
      "  396  102    2  304  244    8  285  354  262    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# display same as above but as converted to int word mappings\n",
    "example_tokenised=vectorize_layer(text_data[2])\n",
    "print(example_tokenised.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebaa419-5325-4a5e-935f-3e87967268df",
   "metadata": {},
   "source": [
    "**INTERIM CONCLUSION**\n",
    "\n",
    "From the examples above we can see how recipe has been tokenized:\n",
    "- firts two tokes 26,12 a relevant to recipe for\n",
    "- there are 1's in the code, meaning that some words are lost their meaning\n",
    "- it is only 50% if vector represented as words, the other 50% filled with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906c8da-1b5e-4deb-9b50-f2e71cf963d3",
   "metadata": {},
   "source": [
    "### Create Training Dataset\n",
    "\n",
    "prepare_inputs convert the dataset to the MapDataset where each sentnce is splited on to the 2 sets of sequences:\n",
    "- x: contains all words in sentece except last\n",
    "- y: shifted by one left, thus it starts from the 2nd element\n",
    "\n",
    "Thus we will have a sort of tuple `[x,y]` thus when our model will train it will learn relation ships between words as it nos that word `x` the target will be `y`. For example in sentence `The cloud is white` model will learn that `x=The` and `y=cloud` so it will adjust it weight accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60ad6ad-8370-4284-8bda-05b23cfce6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=text_ds.map(prepare_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "198bb99b-4fdd-4978-91f3-d74fc45cee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.map_op._MapDataset"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08ac469a-0eef-42c9-ae64-584b8ae3de8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[[  26   16 1759 ...   82   12    2]\n",
      " [  26   16  187 ... 1023   35    4]\n",
      " [  26   16 5649 ...    3   46  255]\n",
      " ...\n",
      " [  26   16 2130 ...    0    0    0]\n",
      " [  26   16  418 ...    0    0    0]\n",
      " [  26   16  617 ...    6  303    3]]\n",
      " y=[[  16 1759   13 ...   12    2   97]\n",
      " [  16  187   13 ...   35    4   72]\n",
      " [  16 5649   27 ...   46  255 1546]\n",
      " ...\n",
      " [  16 2130  525 ...    0    0    0]\n",
      " [  16  418  298 ...    0    0    0]\n",
      " [  16  617 1481 ...  303    3   18]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 00:12:20.620427: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_ds.take(1):\n",
    "    print(f'x={x}\\n y={y}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95ef3d72-0fc0-4d1a-8a2a-67cc00d05c63",
   "metadata": {},
   "source": [
    "## BUILD LSTM\n",
    "\n",
    "The Embedding matrix visualisation  \n",
    "<img src=\"./images/3blue1brown-embeddings.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "(c) [3Blue1Brown](https://www.youtube.com/watch?v=wjZofJX0v4M)\n",
    "\n",
    "<img src=\"./images/lstm-embeddings.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "(c) [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/): David Foster's book from which has become an inspiration of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd187ad9-b50b-41d8-a46f-fb9daacfe83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=100\n",
    "# Number of LSTM cells\n",
    "N_UNITS=128\n",
    "\n",
    "# Input any size(shape=None) because sentences may varry\n",
    "# Role: Pass a tensor of integer sequences of shape [batch_size,seq_length] to the embeding layer\n",
    "inputs=layers.Input(\n",
    "    shape=(None,),\n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "# An embedding layer is essentially a lookup table that converts each integer token into a vector of length EMBEDDING_DIM\n",
    "# Role: Pass a tensor shape [batch_size,seq_length,EMBEDDING_DIM] to the LSTM\n",
    "x=layers.Embedding(\n",
    "    VOCAB_SIZE,\n",
    "    EMBEDDING_DIM\n",
    ")(inputs)\n",
    "\n",
    "x=layers.LSTM(\n",
    "    N_UNITS,\n",
    "    return_sequences=True\n",
    ")(x)\n",
    "\n",
    "outputs=layers.Dense(\n",
    "    VOCAB_SIZE,\n",
    "    activation='softmax'\n",
    ")(x)\n",
    "\n",
    "lstm=models.Model(inputs,outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cea1f061-ceaa-4ba2-b194-6659ba763237",
   "metadata": {},
   "source": [
    "**INTERIM CONCLUSION**\n",
    "\n",
    "The undetneath mechanics is following:  \n",
    "1. **Input Layer**: This layer accepts a tensor of shape `[batch_size, seq_length] = [32, 201]`, where each element represents a token (word) in a sequence of 201 tokens per batch. The input tensor is then passed to the Embedding layer.\n",
    "   \n",
    "2. **Embedding Layer**: The Embedding layer converts each token in the sequence to a vector of length EMBEDDING_DIM. In our example, each token is mapped to a vector of size 100, so the layer outputs a tensor with shape `[batch_size, seq_length, EMBEDDING_DIM] = [32, 201, 100]`. This transformation allows the model to work with dense, meaningful(i.e. a vector that captures essential information or patterns about the data it represents, in a way thatâ€™s useful for the task at hand) vector representations of words rather than raw integer tokens.\n",
    "\n",
    "*Note*: Tokens with the same ID (i.e., the same word) will be mapped to identical vectors, regardless of where they appearâ€”whether in the same sequence, across different batches, or in the entire corpus. This ensures that each word has a consistent embedding representation.\n",
    "\n",
    "3. **LSTM Layer**: The LSTM (Long Short-Term Memory) layer processes the tensor from the Embedding layer over 201 time steps for  each sequence in the batch, handling each vector from the embedding one at a time. This means:\n",
    "- each vector in the sequence (one token's embedding) is fed to the LSTM as ğ‘¥ğ‘¡, representing the token at the current time step ğ‘¡.\n",
    "- the LSTM outputs a hidden state â„ğ‘¡ at each step, representing the modelâ€™s updated understanding of the sequence up to that point. This hidden state can be thought of as the model's probability prediction for the next token in the sequence. \n",
    "- since LSTM param `return_sequences=True` is set, the LSTM will retain the hidden state â„ğ‘¡ for every token in the sequence. This is essential for backpropagation, allowing the model to adjust each prediction by comparing it to the actual next token stored in \n",
    "ğ‘¦. This setting is necessary for sequence-to-sequence tasks (like predicting the next word for each position in the sentence) rather than a single output. While backpropagation is applied through all time steps, return_sequences=True is specifically for capturing all time-step predictions in the output tensor.\n",
    "- after processing all tokens, the LSTM outputs a tensor of shape `[batch_size, seq_length, N_UNITS] = [32, 201, 128]`, where each element in the sequence now has a 128-dimensional vector representing the hidden state for that position.\n",
    "\n",
    "   <img src=\"./images/lstm.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "4. **Dense Layer**: The Dense layer receives the tensor of shape `[32, 201, 128]` from the LSTM layer. It then calculates probabilities for each word in the vocabulary by:\n",
    "- multiplying each 128-dimensional hidden state vector by a weight matrix of shape `[128, VOCAB_SIZE] = [128, 10000]` and adding a bias vector of size `[10000,]`.\n",
    "- this results in an output of shape `[32, 201, 10000]`, where each 10,000-dimensional vector represents the probability distribution over the vocabulary for the next word. Each probability value represents how likely each word in the vocabulary is to follow the current sequence up to the respective token.\n",
    "The Dense layer combines weights across the Embedding, LSTM, and Dense layers that contribute to predictions. The Dense layerâ€™s main role is to transform each LSTM output to a vocabulary-sized probability distribution using the learned weights.\n",
    "The Dense layerâ€™s weight matrix is initialized at model creation and is getting adjusted during training by backpropagating the loss. Each batch provides feedback to adjust the weights, aligning predictions with true next tokens(i.e. data saved in `y`).\n",
    "  \n",
    "5. **Learning Process**: During training, the Dense layerâ€™s output probabilities for each word are compared to the actual next words in the sequence, forming a \"loss\" that tells the model how far its predictions were from reality. This difference is then backpropagated through the model to adjust the weights, particularly the Dense layerâ€™s weight matrix. These adjusted weights allow the model to improve its predictions for the next word over time, capturing patterns in word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f06109-1bcd-47d9-bdd8-6c4a6464d4dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      â”‚     \u001b[38;5;34m1,000,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚       \u001b[38;5;34m117,248\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)    â”‚     \u001b[38;5;34m1,290,000\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91624fa9-ebb5-452a-a203-3a09e98fd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL=False\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    lstm=models.load_model('./models/lstm', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12483f36-0a10-4ad7-ab97-a44733522532",
   "metadata": {},
   "source": [
    "## TRAIN LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa487e-4dbc-4252-957f-b04a7be2b27a",
   "metadata": {},
   "source": [
    "### Configure Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe25c946-17a4-4721-a96a-963e186ac195",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=losses.SparseCategoricalCrossentropy()\n",
    "lstm.compile('adam',loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3527b-b75d-4876-ab6e-47fe7f0d0d7f",
   "metadata": {},
   "source": [
    "### Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dea36f9-8651-473c-bd22-39b9442442e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(self,index_to_word,top_k=10):\n",
    "        self.index_to_word=index_to_word\n",
    "        self.word_to_index={\n",
    "            word: index for index,word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs=probs ** (1 / temperature)\n",
    "        probs=probs / np.sum(probs)\n",
    "\n",
    "        return np.random.choice(len(probs),p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        start_tokens=[\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token=None\n",
    "        info=[]\n",
    "\n",
    "        while len(start_tokens) < max_tokens and sample_token !=0:\n",
    "            x=np.array([start_tokens])\n",
    "            y=self.model.predict(x,verbose=0)\n",
    "            sample_token, probs= self.sample_from(y[0][-1], temperature)\n",
    "            info.append({'prompt':start_prompt, 'word_probs': probs})\n",
    "            start_tokens.append(sample_token)\n",
    "            start_prompt=start_prompt + ' ' + self.index_to_word[sample_token]\n",
    "\n",
    "        print(f'\\ngenerated text:\\n{start_prompt}\\n')\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate('recipe for', max_tokens=100, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2717a26-8163-477d-8822-dafdad7a125f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac3dbc8-b968-4846-94c2-8081239ad037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:47:27.900081: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-11-01 16:47:27.900123: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-11-01 16:47:27.900159: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1239] Profiler found 1 GPUs\n",
      "2024-11-01 16:47:27.900477: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:137] cuptiGetTimestamp: error 999: \n",
      "2024-11-01 16:47:27.900491: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.\n",
      "2024-11-01 16:47:27.900495: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-01 16:47:27.900500: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1282] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "2024-11-01 16:47:27.900588: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-11-01 16:47:27.900601: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.\n",
      "2024-11-01 16:47:27.900605: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-01 16:47:27.900608: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1373] function cupti_interface_->Finalize()failed with error \n"
     ]
    }
   ],
   "source": [
    "log_dir='./logs/fit/lstm/recipe/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "model_checkpoint_callback=callbacks.ModelCheckpoint(\n",
    "    filepath='./checkpoints/lstm.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "tensorboard_callback=callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e7aa22-8344-41f6-afef-e53d78c6ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator=TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eee7ed-69ee-4f62-ac69-840e9faafd46",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7636e367-3069-461e-ba29-c60db36bd4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:47:28.769867: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/629\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:25\u001b[0m 136ms/step - loss: 9.2076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:47:29.210285: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-11-01 16:47:29.210344: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-11-01 16:47:29.210368: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.210387: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.210391: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.210397: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1282] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "2024-11-01 16:47:29.328934: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-11-01 16:47:29.329046: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.329055: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.329058: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1373] function cupti_interface_->Finalize()failed with error \n",
      "2024-11-01 16:47:29.330587: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.330638: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-11-01 16:47:29.330649: I external/local_xla/xla/backends/profiler/gpu/cupti_collector.cc:540]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2024-11-01 16:47:29.331582: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-11-01 16:47:29.338171: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: ./logs/fit/lstm/recipe/20241101-164727/plugins/profile/2024_11_01_16_47_29/Descartes.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 4.9996\n",
      "generated text:\n",
      "recipe for pea moroccan blackberry \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 126ms/step - loss: 4.9982\n",
      "Epoch 2/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 3.0475\n",
      "generated text:\n",
      "recipe for coat tart - kale wontons with ginger | preheat oven over soaking surface . . bring in a processor or until small heavy broiler ) partially rack until finely stock is crosswise thickens still oil , about 12 minutes . bake occasionally in same 425Â° and then pour into dusted tortillas . with vanilla in medium bowl . cover rack and generously lay racks additions . strudels cooking onions , and bring to small large bowl . reduce low heat until guinness ahead , being oils , adding chile pancetta to medium - high cucumber , 6 garnished\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 3.0472\n",
      "Epoch 3/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 2.4841\n",
      "generated text:\n",
      "recipe for slow crisps mule | butter for 2 minutes . using a cocktail cooker , transfer skins . drain a milk baking lid and all heat . add any flour and 1 / spice it . drain ; transfer to a plate and preheat oven , switching of measuring pot of a food thick - size becomes soft appear ) . strain the blackberries into a large bowl . slip the cooling into a rimmed heavy a large deep sieve , the carrots stage . stir to the ice , the baking powder , and bay juice , making\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 363ms/step - loss: 2.4840\n",
      "Epoch 4/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 2.2403\n",
      "generated text:\n",
      "recipe for â€š - tuna | preheat oven to 350Â°f and make mold in the refrigerator or middle of each packed . remove of fish slices through baking dish in middle of oven - middle . set aside . cut enough patties into 1 cup salt and finely whisk together soy sauce , basil over a medium bowl , then transfer olive oil . rub rice with remaining 1 / 4 cup berries with a metal spatula to pour juice and potatoes . divide between ramekins if necessary with its plates . invert steak with waxed paper and frozen bell\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 2.2402\n",
      "Epoch 5/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - loss: 2.0772\n",
      "generated text:\n",
      "recipe for chocolate chips with vanilla cream cake \" | put oven rack in upper third third and preheat oven to 450Â°f with 2 to 400Â°f . heat nonstick oven to medium - low - low wafers , blanch spinach in an dish mayonnaise in 2 minutes , or until potato is golden brown . remove soaking , a baking sheet until s darker , and crispy 10 minutes . drain . squeeze remaining 1 / 2 cup sugar in a beer over medium - gallon skillet . press onto v and 31 balls , and squeeze foam from shells\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 362ms/step - loss: 2.0772\n",
      "Epoch 6/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.9684\n",
      "generated text:\n",
      "recipe for caviar whipped tart crust surface | place shallow baking pan on baking sheet . toss duck and feta with sugar . scoop out spices mixture to chicken . remove from first 9 - inch heat . cool 10 minutes . place in batches , cut sides down , about 2 tablespoons , 2 to 1 / 4 - inch thick and sticky second bouillon - 30 minutes . let cream stand at room temperature and blend to coat . using sauce , drop sprig and 1 / 4 cup syrup into earl . preheat to blend . mix\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.9683\n",
      "Epoch 7/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.8851\n",
      "generated text:\n",
      "recipe for peach and poached coconut belly | whisk together garlic , bread crumbs , honey , spices , to yeast , and salt in a large bowl with an electric mixer at high speed until dough is very thick and 1 ball , about 2 minutes . add yolks and vanilla mixture over onto a colander , then stir in 1 / 4 of cup batter . pour with a offset spatula , then continue to bake in another medium bowl from pan until cold but not quickly , pulsing firmly on a bouquet immediately , adding 2 tablespoons\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.8851\n",
      "Epoch 8/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 1.8438\n",
      "generated text:\n",
      "recipe for robson | 1 . preheat the oven to 450Â°f . 2 . purÃ©e the tamarind juice , the mustard seeds , honey , the cointreau , water , salt , and salt and pepper to taste ; coarsely ground lime , form into the eggshells . 3 . whisk in the batter and continue to over medium heat or until it begins to 300Â°f . 275Â° the creole bread in separate baking pan . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 360ms/step - loss: 1.8437\n",
      "Epoch 9/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.7925\n",
      "generated text:\n",
      "recipe for raspberry mousse with raspberry sauce | place brioche waffle iron , pressing with enough flour to cover by side of even jar . pierce evenly over bottom of oven . divide soufflÃ© mixture among muffin cups set . sprinkle with 1 / 4 cup cheese , salt , 1 tablespoon pepper , and remaining 2 tablespoons of orange peel . place pumpkin slices in baking sheet ; season to taste with salt and pepper . top with serve . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 364ms/step - loss: 1.7925\n",
      "Epoch 10/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.7626\n",
      "generated text:\n",
      "recipe for onion cloves | whisk 1 of tartar - vegetable oil in small bowl . mix in currants . mix in corn , and walnuts . sprinkle chicken with salt and pepper . whisk vinegar , oil and soy sauce in small bowl . season salad with salt and pepper . season with pepper . spoon remaining mushrooms in each toast . sprinkle with salt and toss to coat . sprinkle with mint leaves . pour wine into large shallow baking sheets . mix panko , and remaining 2 teaspoons butter in thin layer to pan . bake until\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.7626\n",
      "Epoch 11/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 1.7293\n",
      "generated text:\n",
      "recipe for [UNK] purÃ©e relish | in a small bowl stir together mayonnaise , zest , salt , and chile . let jellied until bulgur is smooth and mixture becomes thick but smooth and solids in a bowl ; season with salt to taste . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 358ms/step - loss: 1.7293\n",
      "Epoch 12/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.6972\n",
      "generated text:\n",
      "recipe for quick glass with eggs and szechuan chipotle dressing | preheat oven to 500Â°f . and season the salt and 1 / 4 ounce in a 6 1 / 2 - gallon meat . mix in lettuce leaves and [UNK] . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 361ms/step - loss: 1.6972\n",
      "Epoch 13/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - loss: 1.6674\n",
      "generated text:\n",
      "recipe for carrot - and tomato salsa | wash the potato and place in a jar , pressing the solids with the liquid . squeeze you down the stock to cover as much with a little . taste level tsp . 4 and put the onion in a sealable plastic bag . dust vegetable , making sure it more away during for each side , or until they are almost doubled . if using your fingers , stir together the remaining ingredients . drizzle the olive oil in remaining 1 / 4 cup oil . rub with the remaining tablespoon\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 366ms/step - loss: 1.6674\n",
      "Epoch 14/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 1.6528\n",
      "generated text:\n",
      "recipe for herb matzo brisket | bring broth to boil in pot over medium heat . add mussels ; simmer until tender , about 20 minutes . uncover and mix to blend . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 357ms/step - loss: 1.6528\n",
      "Epoch 15/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.6418\n",
      "generated text:\n",
      "recipe for grilled beef tenderloin burgers | combine all ingredients in small saucepan . sautÃ© for 10 minutes . season with salt and pepper . cool very 20 minutes . whisk crÃ¨me fraÃ®che , 1 / 1 cup cheese and mustard in large bowl ; blend 1 minute . bring herbs , shallots , 3 / 4 cup mozzarella , fish , garlic powder , 2 tbsp / 2 tablespoons cilantro into heavy small pot . add chili - sized pieces . cover and simmer until lamb is tender and reduces by half slightly , about 30 minutes . transfer\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.6418\n",
      "Epoch 16/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.6237\n",
      "generated text:\n",
      "recipe for spicy ham , pieces , and mussels with black wine and sauce | tear off / heart back , then serve at room temperature . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 360ms/step - loss: 1.6237\n",
      "Epoch 17/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - loss: 1.6053\n",
      "generated text:\n",
      "recipe for turkey cheeseburgers with crab - garlic sauce | preheat oven to 375Â°f . toss oregano , baking powder , garlic , and mustard in a small bowl and toss to combine . drizzle with olive oil , season with pepper ; season salad with salt and pepper . arrange on work surface . \n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 360ms/step - loss: 1.6053\n",
      "Epoch 18/25\n",
      "\u001b[1m626/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - loss: 1.5947\n",
      "generated text:\n",
      "recipe for fish steamed onions | peel beets and cut into 1 cup . pat ribs dry and spread on a diagonal around veal . tie inside skin of fat with a stuffing of turkey ( do not slide some supermarkets . ) spoon ribs into small roasting pan . seal chickens and neck , baste stuffing to seal dish . place salmon atop potatoes . pour 2 tablespoons sheet ( threads will rise in 350Â°f . ) place remaining large heavy 1 / 4 - cup patties and to large ovenproof ovenproof pan . roast chicken until lamb is\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.5947\n",
      "Epoch 19/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.5852\n",
      "generated text:\n",
      "recipe for strawberry gratins with fresh pecans | position rack in center of oven and preheat to 350Â°f . butter 13 by 9 x 2 x 9 - inch deep steamer basket and turkey wings and neck turkey with cold water . coat sweet potatoes and mix , lemon juice , allspice , and salt ; stir 1 cup vodka , sugar and cranberry juice to water . mix in onions . melt remaining 1 tablespoon tarragon in large ovenproof pot over medium heat . toast bacon over medium heat until brown and crisp , about 1 minute . transfer\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.5852\n",
      "Epoch 20/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - loss: 1.5667\n",
      "generated text:\n",
      "recipe for pizza dough | combine the ingredients , mixing each chili powder , and oil . set aside for medium - size of the beetroot ( for the rolls for use time ) to prevent drying out ; cover the melon and cut onto the grain into 1 - inch - thick strips . unwrap the mixture evenly , cut out the toasted slices between your hands and roll wieners in a bowl with some of oil water and gently stretch the excess dough to fit and chill , until firm enough to handle , as it doesn '\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 362ms/step - loss: 1.5668\n",
      "Epoch 21/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - loss: 1.5541\n",
      "generated text:\n",
      "recipe for potage saint - o sauce | preheat broiler . put oven rack in middle position and preheat oven to 450Â°f . line baking sheet with foil and 1 . heat a tablespoon oil over low heat ; let stand 5 minutes . preheat a deep [UNK] dish to make 36 ( 1 / 4 inch thick ) . in a bowl and let sit 2 minutes , or until vegetables are tender . transfer to a bowl of ice sauce to measure and add to bowl and mash walnuts with the mixture . transfer to a food processor\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 366ms/step - loss: 1.5541\n",
      "Epoch 22/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.5512\n",
      "generated text:\n",
      "recipe for beef pot chicken , vegetables , balsamic vinegar , and tarragon | bring beef broth to large pot to boil . in large saucepan over high heat , stir 3 tablespoons flour - fry , stirring occasionally , until fennel starts to brown , about 2 minutes . add mussels ; with reserved broth and lemon juice . simmer until sauce is meat is almost heated through , about 25 minutes . strain into medium bowl , pressing on solids with spatula , adding pecorino as much liquid if refrigerated if continuing . with machine running , gradually\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.5512\n",
      "Epoch 23/25\n",
      "\u001b[1m625/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - loss: 1.5284\n",
      "generated text:\n",
      "recipe for stir - roll with mustard butter yogurt sauce | 1 . in a 9 - inch bowl , combine the salt , sherry , butter , olive oil , cumin , paprika , salt , and crushed red pepper flakes , along with the cranberries , salt , and pepper , then with the motor running , gradually add oil to the egg and working in 3 batches , begin to form a paste . add the remaining tablespoon of the dough , drop the batter by at a time , cover with plastic wrap , reflouring hands\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 360ms/step - loss: 1.5286\n",
      "Epoch 24/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.5331\n",
      "generated text:\n",
      "recipe for bourbon brown betty | combine cream , bay leaf and lemon peel in a medium heatproof bowl . bring to a bare simmer , whisking occasionally and then chocolate . between a large plate and allowing some at least 1 / 4 cup . preheat oven to 350Â° . spread the glass and cool slightly , then scoop each potato spear to a deep hole in center of up to work surface . combine 4 garlic cloves by each of the top of each flour mixture , alternating with one of three balls for 2â€“3 at a time\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 363ms/step - loss: 1.5331\n",
      "Epoch 25/25\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 1.5204\n",
      "generated text:\n",
      "recipe for escarole and mesclun gratin supper | preheat oven to 425Â° . sautÃ© onion peels until just tender , about 10 minutes . put on rack and cook artichokes in 4 1 / 2 cup water in boiling salted water , stirring occasionally , until cheese is and golden , 15 to 20 minutes , remove pan from onion . season herb butter generously generously with salt and pepper . transfer to small bowl and discard seeds . microwave salsa verde at high speed until just set , about 10 minutes . cool and bell pepper , 2 1\n",
      "\n",
      "\u001b[1m629/629\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 365ms/step - loss: 1.5204\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=./models/lstm.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensrnv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tensrnv/lib/python3.11/site-packages/keras/src/saving/saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=./models/lstm."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS=25\n",
    "\n",
    "lstm.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback,\n",
    "              tensorboard_callback,\n",
    "              text_generator]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6116ff6-ef5c-4ff9-b36e-9358d9bc8915",
   "metadata": {},
   "source": [
    "**INTERIM CONCLUSION**\n",
    "\n",
    "<img src=\"./images/lstm-loss.png\" width=\"300\" height=\"400\">\n",
    "\n",
    "On the presented graph we see that our model's training loss decreases consistently, which indicates that the LSTM model is learning effectively. After 20 epochs, the loss appears to flatten out, approaching a value around 1.5, suggesting the model has converged or reached a plateau in training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e59fa621-f67f-4ea3-85c6-7615c1821e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save('./models/lstm/lstm.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd6f12-8d37-4423-b012-211b38e265ce",
   "metadata": {},
   "source": [
    "## TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e90f1d8f-cfa0-4651-ad60-0da25e6626cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "recepie for roasted vegetables | chop 1/ pea purÃ©e in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info=text_generator.generate(\n",
    "    'recepie for roasted vegetables | chop 1/', max_tokens=10, temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28c706b-3101-4975-b7f8-ccaf4925eb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "recepie for roasted vegetables | chop 1 / 2 teaspoon salt and 1 / 4 teaspoon pepper , and 1 / 4 teaspoon salt , and 1 / 4 teaspoon salt , and 1 / 2 teaspoon of salt and 1 / 2 teaspoon pepper in a food processor until it is smooth . add the butter and stir until it is smooth . add the remaining 1 / 2 cup of the remaining 1 tablespoon of the reserved juice , the lemon juice , and the salt and simmer the mixture , stirring , for 1 minute . stir\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info=text_generator.generate(\n",
    "    'recepie for roasted vegetables | chop 1 /', max_tokens=100, temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed564d9b-864b-4679-8cf7-e2264f939577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "recipe for chockolate ice cream | in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info=text_generator.generate(\n",
    "    'recipe for chockolate ice cream |', max_tokens=7, temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc4d47d1-2efc-48fd-831f-f6a03c74742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT: recipe for chockolate ice cream |\n",
      "combine:   \t19.51%\n",
      "in:   \t12.01%\n",
      "stir:   \t7.83%\n",
      "1:   \t6.83%\n",
      "bring:   \t6.52%\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_probs(info,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85adad9b-ca40-4032-bb01-e3031eb4b3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "recipe for chockolate ice cream | combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info=text_generator.generate('recipe for chockolate ice cream |', max_tokens=7, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e68e6045-be6b-4e5e-8723-7920d41f48e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT: recipe for chockolate ice cream |\n",
      "combine:   \t90.01%\n",
      "in:   \t7.94%\n",
      "stir:   \t0.94%\n",
      "1:   \t0.47%\n",
      "bring:   \t0.37%\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_probs(info,vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003f06eb-3cce-43b9-a939-bebab03406f0",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/): David Foster's book from which has become an inspiration of this notebook.\n",
    "2. [David Foster](https://github.com/davidADSP): GitHub page\n",
    "3. [David Foster (Keynote) - Generative Deep Learning -Key To Unlocking Artificial General Intelligence?](https://www.youtube.com/watch?v=rHLf78CmNmQ): David's video session at Youtube regarding some key concepts has written in his book\n",
    "4. [Understanding LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): Christopher Olah's blog where he is explaining LSTM networks.\n",
    "5. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/): Anfdrej Karpathy's blog where he is demostrates some cool features of RNNs.\n",
    "6. [Long Short-Term Memory (LSTM), Clearly Explained](https://www.youtube.com/watch?v=YCzL96nL7j0): Josh Stamer's StatQuest YouTube channel with amazing explanation and visualisation of LSTM key concepts.\n",
    "7. [Keras LSTM source code](https://github.com/keras-team/keras/blob/master/keras/src/layers/rnn/lstm.py): Link to Keras LSTM layer source code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
